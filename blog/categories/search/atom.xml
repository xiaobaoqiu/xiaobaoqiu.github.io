<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Search | xiaobaoqiu Blog]]></title>
  <link href="http://xiaobaoqiu.github.io/blog/categories/search/atom.xml" rel="self"/>
  <link href="http://xiaobaoqiu.github.io/"/>
  <updated>2017-02-04T17:24:59+08:00</updated>
  <id>http://xiaobaoqiu.github.io/</id>
  <author>
    <name><![CDATA[xiaobaoqiu]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Crate文档翻译]]></title>
    <link href="http://xiaobaoqiu.github.io/blog/2016/08/20/cratewen-dang-fan-yi/"/>
    <updated>2016-08-20T12:37:02+08:00</updated>
    <id>http://xiaobaoqiu.github.io/blog/2016/08/20/cratewen-dang-fan-yi</id>
    <content type="html"><![CDATA[<p>最近准备开始抽个人时间着手翻译Crate的一些文档，主要是考虑两个方面：</p>

<pre><code>1.使用了1年的Crate，还没有比较全面的看Crate文档，很多特性肯定还不知道；
2.Crate被越来越多的人使用，中文文档肯定ui别人有帮助；
</code></pre>

<p>在 github 上开了个项目(其实很早就建工程了，一直没行动，也想通过这种方式驱动自己：先把牛B吹出去&hellip;)：</p>

<pre><code>https://github.com/xiaobaoqiu/crate_doc_cn.git
</code></pre>

<p>主要分三大块：</p>

<pre><code>1.Crate Server相关文档；
2.Crate Client相关文档；
3.Crate其他文档；
</code></pre>

<p>欢迎监督和参与。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lucene整体架构]]></title>
    <link href="http://xiaobaoqiu.github.io/blog/2015/09/09/lunecezheng-ti-jia-gou/"/>
    <updated>2015-09-09T22:19:51+08:00</updated>
    <id>http://xiaobaoqiu.github.io/blog/2015/09/09/lunecezheng-ti-jia-gou</id>
    <content type="html"><![CDATA[<ul>
<li><a href="#1.%E6%BA%90%E7%A0%81%E7%BB%93%E6%9E%84">1.源码结构</a></li>
<li><a href="#2.Lucene%E4%BD%BF%E7%94%A8Demo">2.Lucene使用Demo</a>

<ul>
<li><a href="#2.1%20%E4%B8%BB%E8%A6%81%E7%B1%BB">2.1 主要类</a></li>
</ul>
</li>
<li><a href="#3.%E5%BB%BA%E7%AB%8B%E7%B4%A2%E5%BC%95%E6%B5%81%E7%A8%8B">3.建立索引流程</a></li>
<li><a href="#4.%E6%90%9C%E7%B4%A2%E6%B5%81%E7%A8%8B">4.搜索流程</a></li>
</ul>


<p>根据上篇文章,我们大致知道Lucene创建索引和搜索的一个过程:</p>

<p><img src="/images/lucene/Lucene_index_and_search.jpg"></p>

<p>这里主要分析一下Lucene的代码结构,包括理由Lucene API实现一个简单建立索引和搜索的小例子.</p>

<p>由于我们的服务器上crate是基于Lucene 4.10.3版本.所以我这里采用的Lucene版本也是4.10版本,小版本是4.10.4,是Lucene 4的最后一个版本,大约2014年9月推出,还比较新鲜.目前已经到了5.3版本.</p>

<h2 id="1.源码结构">1.源码结构</h2>


<p>首先看一下Lucene的源码结构,Lucene-core主要代码包结构如下:</p>

<p><img src="/images/lucene/Lucene_source_struct.png"></p>

<p>每个包下面都有package.html文件介绍包的作用.各个包的作用如下:</p>

<pre><code>1.analysis
分词模块,包含将文本转化为可索引化/可搜索化的词元(convert text into indexable/searchable tokens)的API和实现.包括org.apache.lucene.analysis.Analyzer和其相关类.
2.codecs
包含自定义底层索引的编码和索引结构的模块,还包括索引的压缩,索引版本管理等.
3.document
定义了被索引和搜索内容的用户层面的逻辑定义,就是我们熟悉的Document,各种类型的Filed及其属性定义等.同事也提供了一下org.apache.lucene.document.Document和org.apache.lucene.index.IndexableField相关的工具类.
4.index
包含了索引维护(创建,更新,删除)和访问(读)的逻辑.包括我们熟悉的IndexReader,IndexWriter等.
5.search
索引的搜索.包括相似度的定义(similarities包),权重计算,文档评分计算,布尔模型,搜索结果搜集等逻辑.
6.store
索引底层二进制数据的读写,包括索引文件的Buffered的流的实现等,还包括一些限流策略的实现(RateLimiter)等.
7.util
工具类,如排序器,数学工具类,优先级队列等工具的实现等.
</code></pre>

<p>包基本上和本文开头的图对应起来,其中的QueryParser再Lucene4中是一个单独的jar包:</p>

<p><img src="/images/lucene/Lucene_code_package_usage.jpg"></p>

<h2 id="2.Lucene使用Demo">2.Lucene使用Demo</h2>


<p>首先上一个Lucene的简单的Demo,基本上是参考官网提供的Demo,自己手动修改了点,作用就是索引一个目录下的文本文件,在此基础上提供搜索服务.其中Indexer用于创建索引,Searcher作为搜索入口,接受用户输入的Query词并执行搜索.</p>

<pre><code>/**
 * 创建索引
 *
 * @author: xiaobaoqiu  Date: 15-9-8 Time: 下午4:04
 */
public final class Indexer {

    public static void main(String[] args) {
        indexDocs(CommonConfig.SOURCE_FILE_DIR, CommonConfig.INDEX_FILE_DIR, true);
    }

    /**
     * 构建磁盘索引
     *
     * @param dirPath 原始文件
     * @param indexPath 索引文件存放路径
     * @param create    创建or更新
     */
    public static void indexDocs(String dirPath, String indexPath, boolean create) {
        try {
            Directory dir = FSDirectory.open(new File(indexPath));
            Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_4_10_0);
            IndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_4_10_0, analyzer);

            if (create) {
                iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);
            } else {
                iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);
            }

            IndexWriter writer = new IndexWriter(dir, iwc);
            doIndexDocs(writer, new File(dirPath));

            System.out.println("Build index finished...");
            writer.close();
        }catch (IOException ioe) {
            //do something...
        }
    }

    static void doIndexDocs(IndexWriter writer, File file) throws IOException {
        if (!file.canRead()) {
            System.out.println("文件不可读:" + file.getAbsolutePath());
            return;
        }

        //文件夹则索引文件夹下的索引文件
        if (file.isDirectory()) {
            String[] files = file.list();
            if (files != null) {
                for (int i = 0; i &lt; files.length; i++) {
                    doIndexDocs(writer, new File(file, files[i]));
                }
            }
        } else {
            doIndexFile(writer, file);
        }
    }

    /**
     * 索引一个文件
     *
     * @param writer
     * @param file
     */
    private static void doIndexFile(IndexWriter writer, File file) throws IOException{
        //单个文件
        FileInputStream fis;
        try {
            fis = new FileInputStream(file);
        } catch (FileNotFoundException fnfe) {
            //do nothing
            return;
        }

        try {
            // 创建一个新的空的Document
            Document doc = new Document();

            //文件路径字段
            doc.add(new StringField(CommonConfig.PATH_FIELD_NAME, file.getPath(), Field.Store.YES));
            //文件最后修改时间字段
            doc.add(new LongField(CommonConfig.MODIFIED_FIELD_NAME, file.lastModified(), Field.Store.NO));
            //文件内容,不存储原始串
            doc.add(new TextField(CommonConfig.CONTENTS_FIELD_NAME, new BufferedReader(new InputStreamReader(fis, StandardCharsets.UTF_8))));

            //新建索引 or 更新索引
            if (writer.getConfig().getOpenMode() == IndexWriterConfig.OpenMode.CREATE) {
                writer.addDocument(doc);
            } else {
                writer.updateDocument(new Term("path", file.getPath()), doc);
            }
        } finally {
            fis.close();
        }
    }
}

/**
 * 查询索引
 * 接受用户输入作为Query词
 *
 * @author: xiaobaoqiu  Date: 15-9-8 Time: 下午4:09
 */
public final class Searcher {

    public static void main(String[] args) throws Exception {
        search(CommonConfig.INDEX_FILE_DIR, CommonConfig.CONTENTS_FIELD_NAME);
    }

    public static void search(String indexPath, String field) throws Exception {
        IndexReader reader = DirectoryReader.open(FSDirectory.open(new File(indexPath)));
        IndexSearcher searcher = new IndexSearcher(reader);
        Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_4_10_0);
        QueryParser parser = new QueryParser(Version.LUCENE_4_10_0, field, analyzer);

        //接受用户输入
        BufferedReader in = new BufferedReader(new InputStreamReader(System.in, StandardCharsets.UTF_8));
        String queryString = null;
        while (StringUtils.isNotEmpty(queryString = in.readLine())) {
            Query query = parser.parse(queryString.trim());

            doSearch(searcher, query);
        }
        reader.close();
    }

    public static void doSearch(IndexSearcher searcher, Query query) throws IOException {
        // 搜索 CommonConfig.SEARCH_MAX_ITEM_COUNT 个数据
        TopDocs results = searcher.search(query, CommonConfig.SEARCH_MAX_ITEM_COUNT);
        ScoreDoc[] hits = results.scoreDocs;
        int numTotalHits = results.totalHits;
        System.out.println(numTotalHits + " total matching documents");

        if (ArrayUtils.isEmpty(hits)) return;

        for (ScoreDoc hit : hits) {
            System.out.println();
            Document hitDoc = searcher.doc(hit.doc);
            System.out.println(CommonConfig.PATH_FIELD_NAME + " = " + hitDoc.get(CommonConfig.PATH_FIELD_NAME) +
                    "\n" + CommonConfig.MODIFIED_FIELD_NAME + " = " + hitDoc.get(CommonConfig.MODIFIED_FIELD_NAME) +
                    "\n" + CommonConfig.CONTENTS_FIELD_NAME + " = " + hitDoc.get(CommonConfig.CONTENTS_FIELD_NAME));
        }
    }
}
</code></pre>

<p>输入的文件及生成的索引:</p>

<p><img src="/images/lucene/Lucene_demo_input_output.png"></p>

<p>建立索引的基本过程如下:</p>

<pre><code>1.FSDirectory定义索引文件存放目录
2.选择分词使用的Analyzer.
3.选择索引文件版本信息Version,将其和Analyzer组成IndexWriterConfig.
4.利用IndexWriterConfig生成一个IndexWriter,用于索引文件的变更;
5.根据原始文件生成一些列的Document,每个文档中包含多个Field,设置每个Field是否分词,释放存储等属性;
6.使用IndexWriter将文档写入索引文件.
</code></pre>

<p>搜索的基本过程如下:</p>

<pre><code>1.根据索引文件的目录生成一个IndexReader,将索引文件读到内存中.
2.生成一个IndexSearcher用于搜索;
3.选择一个分词器;
4.定义用户输入Query的解析器QueryParser;
5.接受用户输入,执行搜索过程,得到搜索结果用TopDocs表示,其中包含了命中文档的数目以及命中文档的id集合;
6.根据文档id获取对应的文档,从文档中获取各个Field信息;
</code></pre>

<h4 id="2.1 主要类">2.1 主要类</h4>


<p>首先介绍一下其中涉及到的主要的类:</p>

<ul>
<li><p>FSDirectory
Lucene文件操作都是通过Directory实现，其中有一个比较特殊的方法sync，用来保证任何对文件的写都能够得到永久保存。Lucene使用这个来保证正确提交索引数据变更，也能够防止机器或者操作系统级别的错误影响(corrupting)索引数据.</p></li>
<li><p>Analyzer,StandardAnalyzer</p></li>
<li>Version</li>
<li>IndexWriterConfig</li>
<li>IndexWriter</li>
<li>Document</li>
<li>StringField,LongField,TextField</li>
<li>IndexReader,DirectoryReader</li>
<li>IndexSearcher</li>
<li>QueryParser</li>
<li>Query</li>
<li>TopDocs</li>
<li>ScoreDoc</li>
</ul>


<h2 id="3.建立索引流程">3.建立索引流程</h2>




<h2 id="4.搜索流程">4.搜索流程</h2>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lucene入门]]></title>
    <link href="http://xiaobaoqiu.github.io/blog/2015/09/03/luceneru-men/"/>
    <updated>2015-09-03T14:13:54+08:00</updated>
    <id>http://xiaobaoqiu.github.io/blog/2015/09/03/luceneru-men</id>
    <content type="html"><![CDATA[<p>Crate再项目中扮演越来越重要的角色.但是很多时候我对Crate,包括其下层的ElasticSearch以及Lucene都是一知半解,甚至可以说是完全不懂.因为没看过源代码.所以这段时间会集中学习一下Lucene和ElasticSearch的源码.</p>

<p>从Lucene开始.会包含以下内容:</p>

<pre><code>1.Lucene简介;
2.Lunece整体架构;
3.Lucene的存储;
4.Lucene的搜索;
5.Lucene其他功能,如高亮等;
</code></pre>

<p>这里从Lucene简介开始.第一次使用Lucene已经是两年前入职的时候,写了个简单的爬虫,抓取999网的疾病信息并提供一个简单的搜索入口.这里主要弄懂以下几个问题:</p>

<pre><code>1.信息检索
2.Lucene是什么;
3.Lucene建立索引的大致过程;
4.Lucene搜索的大致过程;
</code></pre>

<p>Lucene官网: <a href="http://lucene.apache.org/">http://lucene.apache.org/</a></p>

<p>在Lucene基础上衍生出很多搜索框架,下面是这些框架的对比:<a href="http://stackoverflow.com/questions/2271600/elasticsearch-sphinx-lucene-solr-xapian-which-fits-for-which-usage">http://stackoverflow.com/questions/2271600/elasticsearch-sphinx-lucene-solr-xapian-which-fits-for-which-usage</a></p>

<h2>1.信息检索</h2>

<p>信息检索简单讲就是从一堆数据中找到你需要的数据,数据的来源可能是文本或者网络,甚至别人说的话或者声波等等.比如:</p>

<ul>
<li>从一个Excel表格中找到你的名字;</li>
<li>从学生成绩标表中找到不及格的学生;</li>
<li>从图书馆里面找Java编程相关的书籍;</li>
<li>从网络搜索Lucene相关知识;</li>
<li>老师点名时候喊到;</li>
</ul>


<p>人类信息检索也是一个不断发展的过程,下面这个例子简单的诠释了信息检索的改善过程:</p>

<ol>
<li>一个书生想从书屋(假设1000本书)里找一本书他能做的就是,从第一本书开始一本一本的往后找,直到找到他要的那本书.如果很不幸,书屋里面每这本书,那么他就必须翻遍所以的书,很可能他找一遍需要10个小时;</li>
<li>经历过几次这样痛苦的找书经历之后,书生开始想招了,将书分类(假设10个分类),文学类在第一个书架子,史学类再第二个书架子&hellip;从此以后,他找书比以前方便多了.找一本书最多只需要翻一个架子,时间缩短到1个小时;但是带来的成本是:必须将书正确的放在其正确的架子上;</li>
<li>每次翻遍一个架子,书生觉得还是比较麻烦,于是他给每本书加了一个编号,将书按照编号排列好,并将所以的书名和编号的对应关系记录到一个单独的册子上.下次他找书只需要在册子上找就可以了(只需要看册子上的1000个书名),找到书名对应的编号,然后就能快速的找到书,时间只需要10分钟.成本是书必须按照需要排列;</li>
<li>10分钟还是有点长,书生于是决定再改善,结合2和3的改进措施.书按照各种类型放在不同的架子,每个架子的书单独编号,比如文学1,文学2&hellip;.册子上也按照书种类进行分开登记.这样找一本书只需要看一个分类下的书名(100个),只需要1分钟.</li>
</ol>


<p>我们生活中的数据分为:结构化数据和非结构化数据.结构化数据指具有固定格式或有限长度的数据,如数据库或者Excel表等.非结构化数据指不定长或无固定格式的数据,如邮件,word文档等.</p>

<p>计算机的产生对信息检索产生了质的变化,帮我们做了很多简单却耗时的工作.在计算机检索的世界里面我们已知了很多技术:</p>

<pre><code>1.对链表的搜索,我们采用从头到位逐项比较,O(N)的时间复杂度;
2.对于有序数组,我们可以采用二分搜索(其实就是二叉树,通常称之为二叉查找树),能够达到O(logN),底数是2;
3.多叉数的查找比二叉树更快,参考Tire树,B树等数据结构,O(logN),底数大于2;
4.Hash表的查找理论上是O(1)的时间复杂度;
</code></pre>

<p>除了顺序查找,其他几种检索都非常的快,但是这种快都是有维护成本的,比如二分搜索需要保证数据有序,因此新来的数据的插入成本会很高,包括删除数据的成本也会很高.但是其带来的检索效率的提升是非常大的.针对结构化数据,我们往往很简单的实现数据的快速搜索,比如数据库就是使用B树来达到快速搜索的目的.</p>

<p>但是现实中99%以上的原始数据都是混乱无须的非结构化数据.当数据量很大的时候,检索成为一个极其困难的问题(量变导致质变),即使对于计算很快的计算机而言.比如一个简单的字符串查找,假设在1页书的内容(约1000字)中查找一个单词可能需要1毫秒,那么在一本书的内容中(约50W词)中查找一个名字可能需要0.5秒,在1W本书(约50亿个单词)中查找这个单词需要5000秒,也就是接近1个半小时;假设每分钟要找一次呢?不敢想象.</p>

<p>在当今网络及其发达的今天,已经很多的公司的数据量超过了1W本书的信息量.这时候无序信息的全文检索显得极其重要.因此也催生了诸如Google,Yahoo和Baidu这些搜索巨头.</p>

<p>索引是全文搜索的基础,也是现代搜索引擎的核心,建立索引的过程就是把源数据处理成非常方便查询的索引文件的过程.你可以把索引想象成这样一种数据结构,他能够使你快速的随机访问存储在索引中的关键词,进而找到该关键词所关联的文档.</p>

<p>还是以书的例子来说,假设书前三页的内容:</p>

<pre><code>//page1:
如何使用Lucene完善搜索

//page2
Lucene数据如何存储

//page3
Lucene如何实现搜索

//page4
作者:肖宝秋
</code></pre>

<p>我们首先使用分词软件见这四页的内容分词,得到以下结果:</p>

<pre><code>//page1:
如何, 使用, Lucene, 完善, 搜索

//page2
Lucene, 数据, 如何, 存储

//page3
Lucene, 如何, 实现, 搜索
作者, 肖宝秋
</code></pre>

<p>建立词&ndash;>文档的映射关系:</p>

<pre><code>如何 --&gt; page1, page2, page3
使用 --&gt; page1
Lucene --&gt; page1, page2, page3
完善 --&gt; page1
搜索 --&gt; page1, page3
数据 --&gt; page2
存储 --&gt; page2
实现 --&gt; page3
作者 --&gt; page4
肖宝秋 --&gt; page4
</code></pre>

<p>现在搜索"Lucene"的时候,我知道在page1,page2,page3中出现了.搜索"作者"的时候,我知道在page4出现了.</p>

<p>另外一个支撑现代搜索理论的既定现实是:词的数量是有限的,而且非常有限.比如常用的中文词就几万个.</p>

<h2>2.Lucene是什么</h2>

<p>简单讲,Lucene是一个高效的,基于Java的全文检索工具包,它不是一个完整的搜索应用程序,而是为你的应用程序提供索引和搜索功能.Lucene是Apache家族中的一个开源项目.也是目前最为流行的基于 Java 开源全文检索工具包.</p>

<p>Lucene 能够为文本类型的数据建立索引,所以你只要能把你要索引的数据格式转化的文本的,Lucene 就能对你的文档进行索引和搜索.比如你要对一些 HTML 文档,PDF 文档进行索引的话你只需要把 HTML 文档和 PDF 文档转化成文本格式的,然后将转化后的内容交给Lucene进行索引,然后把创建好的索引文件保存到磁盘或者内存中,最后根据用户输入的查询条件在索引文件上进行查询.不指定要索引的文档的格式也使Lucene能够几乎适用于所有的搜索应用程序.</p>

<p>下图展示搜索应用程序和Lucene之间的关系,也描述了Lucene的两个核心过程:建立索引和通过索引检索</p>

<p><img src="/images/lucene/lucene_and_application.jpg"></p>

<h4>2.1 Lucene优点</h4>

<p>Lucene作为一个全文搜索引擎,其具有如下突出的优点:</p>

<ul>
<li>1.索引文件格式独立于应用平台.Lucene定义了一套以八字节为基础的索引文件格式,使得兼容系统和不同平台的应用能够共享建立的索引文件.</li>
<li>2.在传统全文检索引擎的倒序索引的基础上实现了分块索引,能够针对新的文件建立小文件索引,提升索引速度.然后通过与原有的索引的合并,达到优化的目的.</li>
<li>3.优秀的面向对象的系统架构,使得对于Lucene扩展的学习难度降低,方便加入新功能.</li>
<li>4.设计了独立于语言和文件格式的文本分析接口,索引器通过接受token流完成索引文件的创立,用户扩展新的语言和文件格式,只需要实现文本分析的接口.</li>
<li>5.已经默认实现了一套强大的查询引擎,用户无须自己编写代码即可使系统获得强大的查询能力,Lucene的查询实现中默认实现了布尔操作、模糊查询、分组查询.</li>
</ul>


<p>参考: <a href="http://lucene.apache.org/core/index.html">http://lucene.apache.org/core/index.html</a></p>

<h4>2.2 倒排索引</h4>

<p>现代搜索引擎基本都是基于倒排索引(反向索引).因为搜索问题本质就是求解"哪文档包含搜索词"这个问题.</p>

<p>简单讲一下什么是倒排索引,简单讲就是字符串到文件的映射关系.假设我的文档集合里面有100篇文档,为了方便表示,我们为文档编号从1到100,得到下面的结构:</p>

<p><img src="/images/lucene/inverted_index.jpg"></p>

<p>左边保存的是一系列字符串,称为词典.每个字符串都指向包含此字符串的文档(Document)链表,此文档链表称为倒排表.</p>

<p>有了索引,便使保存的信息和要搜索的信息一致,可以大大加快搜索的速度.比如说,我们要寻找既包含字符串“lucene”又包含字符串“solr”的文档,我们只需要以下几步:</p>

<ol>
<li>取出包含字符串lucene的文档链表.</li>
<li>取出包含字符串solr的文档链表.</li>
<li>通过合并链表,找出既包含lucene又包含solr的文件.</li>
</ol>


<p><img src="/images/lucene/inverted_index_merge.jpg"></p>

<h2>3.Lucene建立索引</h2>

<p>Lucene的建立索引包括以下几个过程:</p>

<h4>3.1.原始文档</h4>

<p>一些要索引的原文档(Document).包括Html文档,PDF文档,MC Word文档等.而Lucene是只能处理文本文档.</p>

<p>庆幸的是现在很多工具帮我们处理这些问题.如Apache的Tika, 官网:<a href="http://tika.apache.org/">http://tika.apache.org/</a></p>

<h4>3.2.分词</h4>

<p>确定输入的文本之后首先将文本传递给分词组件,分词组件(Tokenizer)会做以下几件事情(此过程称为Tokenize):</p>

<pre><code>1. 将文档分成一个一个单独的单词;
2. 去除标点符号;
3. 去除停词(Stop word);
</code></pre>

<p>停词(Stop word)就是一种语言中最普通的一些单词,由于没有特别的意义,因而大多数情况下不能成为搜索的关键词,因而创建索引时,这种词会被去掉而减少索引的大小.英语中挺词(Stop word)如:the,a,this,is等.对于每一种语言的分词组件(Tokenizer),都有一个停词(stop word)集合.经过分词(Tokenizer)后得到的结果称为词元(Token).</p>

<h4>3.3 语言处理</h4>

<p>分词之后将词元传递给语言处理模块,语言处理组件(linguistic processor)主要是对得到的词元(Token)做一些同语言相关的处理.</p>

<p>比如对于英语,语言处理组件(Linguistic Processor)一般做以下几点:</p>

<pre><code>1. 变为小写(Lowercase).
2. 将单词缩减为词根形式,如cars到car等.这种操作称为:stemming.
3. 将单词转变为词根形式,如“drove”到“drive”等.这种操作称为:lemmatization.
</code></pre>

<p>语言处理组件(linguistic processor)的结果称为词(Term).</p>

<h4>3.4 索引</h4>

<p>语言处理之后将词给索引模块,索引模块主要做以下几个事情:</p>

<pre><code>1. 利用得到的词(Term)创建一个字典.
即Term1--&gt;doc1, Term2--&gt;doc1这种映射关系
2. 对字典按字母顺序进行排序.
3. 合并相同的词(Term)成为文档倒排链表.
</code></pre>

<p>倒排链表的格式如下:</p>

<pre><code>Term1,Document Frequency  --&gt; Doc1,Frequency1 ; Doc2,Frequency2
</code></pre>

<p>Document Frequency 即文档频次,表示总共有多少文件包含此词(Term),即链表中文档数目.
Frequency 即词频率,表示此文件中包含了几个此词(Term).</p>

<p>下面是一个建立索引的例子,输入文本如下:</p>

<pre><code>文件一:Students should be allowed to go out with their friends, but not allowed to drink beer.
文件二:My friend Jerry went to school to see his students but found them drunk which is not allowed.
</code></pre>

<p>经过分词(Tokenizer)后得到的结果称为词元(Token):</p>

<pre><code>Students,allowed,go,their,friends,allowed,drink,beer,My,friend,Jerry,went,school,see,his,students,found,them,drunk,allowed
</code></pre>

<p>经过语言处理,得到的词(Term)如下:</p>

<pre><code>student,allow,go,their,friend,allow,drink,beer,my,friend,jerry,go,school,see,his,student,find,them,drink,allow
</code></pre>

<p>最后得到的倒排索引表如下:</p>

<p><img src="/images/lucene/posting_list.jpg"></p>

<p>所以对词allow来讲,总共有两篇文档包含此词(Term),从而词(Term)后面的文档链表总共有两项,第一项表示包含allow的第一篇文档,即1号文档,此文档中,allow出现了2次,第二项表示包含allow的第二个文档,是2号文档,此文档中,allow出现了1次.</p>

<h2>4.Lucene搜索</h2>

<p>有了倒排索引,我们就可以做搜索了,但是事实上Lucene的搜索过程也不简单.</p>

<p>简单来讲,搜索解决的问题是:找到和搜索词相关性最好的文档.细分的这个问题:</p>

<pre><code>1.找到相关文档;
2.评价搜索词和文档的相关性;
</code></pre>

<p>Lucene的搜索包括以下几个过程.</p>

<h4>3.1.用户输入</h4>

<p>查询语句同我们普通的语言一样,也是有一定语法的.查询语句的语法根据全文检索系统的实现而不同.最基本的有比如:AND, OR, NOT等.举个例子,用户输入语句:lucene AND learned NOT hadoop.说明用户想找一个包含lucene和learned然而不包括hadoop的文档.</p>

<p>目前看来,Google和Baidu都不支出这种语法:</p>

<p><img src="/images/lucene/Google_Search.png"></p>

<p><img src="/images/lucene/Baidu_Search.png"></p>

<h4>3.2.词法分析,语法分析,语言处理</h4>

<p>由于查询语句有语法,因而也要进行语法分析,语法分析及语言处理.</p>

<ul>
<li><p>词法分析主要用来切词,识别单词和关键字.通常建立索引的切词组件和对搜索词的切词组件一致.</p></li>
<li><p>语法分析主要是根据查询语句的语法规则来形成一棵语法树.比如例子:lucene AND learned NOT hadoop形成的语法树如下:</p>

<p>  <img src="/images/lucene/Token_tree.jpg"></p></li>
<li><p>语言处理同索引过程中的语言处理几乎相同.如learned变成learn等.经过第三步,我们得到一棵经过语言处理的语法树.</p>

<p> <img src="/images/lucene/Term_Tree.jpg"></p></li>
</ul>


<h4>3.3 搜索索引</h4>

<p>此步骤有分几小步:</p>

<ul>
<li>在反向索引表中,分别找出包含lucene,learn,hadoop的文档链表.</li>
<li>对包含lucene,learn的链表进行合并操作,得到既包含lucene又包含learn的文档链表.</li>
<li>将此链表与hadoop的文档链表进行差操作,去除包含hadoop的文档,从而得到既包含lucene又包含learn而且不包含hadoop的文档链表.此文档链表就是我们要找的文档.</li>
</ul>


<h4>3.4 结果文档排序</h4>

<p>虽然在上一步,我们得到了想要的文档,然而对于查询结果应该按照与查询语句的相关性进行排序,越相关者越靠前.</p>

<p>通常会把查询语句看作一片短小的文档,对文档与文档之间的相关性(relevance)进行打分(scoring),分数高的相关性好,就应该排在前面.</p>

<p>对于文档之间的关系,不同的Term重要性不同,比如对于本篇文档,search, Lucene, full-text就相对重要一些,this, a , what可能相对不重要一些.所以如果两篇文档都包含search, Lucene,fulltext,这两篇文档的相关性好一些,然而就算一篇文档包含this, a, what,另一篇文档不包含this, a, what,也不能影响两篇文档的相关性.</p>

<p>找出词(Term)对文档的重要性的过程称为计算词的权重(Term weight)的过程.计算词的权重(term weight)有两个参数,第一个是词(Term),第二个是文档(Document).词的权重(Term weight)表示此词(Term)在此文档中的重要程度,越重要的词(Term)有越大的权重(Term weight),因而在计算文档之间的相关性中将发挥更大的作用.</p>

<p>下面分析这两个过程:</p>

<ul>
<li>计算权重(Term weight).</li>
</ul>


<p>影响一个词(Term)在一篇文档中的重要性主要有两个因素:</p>

<pre><code>1.Term Frequency (tf):即此Term在此文档中出现了多少次. tf越大说明越重要.
2.Document Frequency (df):即有多少文档包含次Term. df越大说明越不重要.
</code></pre>

<p>很容易理解,词(Term)在文档中出现的次数越多,说明此词(Term)对该文档越重要,如“搜索”这个词,在本文档中出现的次数很多,说明本文档主要就是讲这方面的事的.然而在一篇英语文档中,this出现的次数更多,就说明越重要吗？不是的,这是由第二个因素进行调整,第二个因素说明,有越多的文档包含此词(Term), 说明此词(Term)太普通,不足以区分这些文档,因而重要性越低.</p>

<p>简单公式如下:</p>

<p><img src="/images/lucene/Term_Weight.png"></p>

<ul>
<li>根据Term之间的关系得到文档相关性.</li>
</ul>


<p>我们把文档看作一系列词(Term),每一个词(Term)都有一个权重(Term weight),不同的词(Term)根据自己在文档中的权重来影响文档相关性的打分计算.</p>

<p>于是我们把所有此文档中词(term)的权重(term weight) 看作一个向量.</p>

<pre><code>Document = {term1, term2, …… ,term N}
Document Vector = {weight1, weight2, …… ,weight N}
</code></pre>

<p>同样我们把查询语句看作一个简单的文档,也用向量来表示.</p>

<pre><code>Query = {term1, term 2, …… , term N}
Query Vector = {weight1, weight2, …… , weight N}
</code></pre>

<p>我们把所有搜索出的文档向量及查询向量放到一个N维空间中,每个词(term)是一维:</p>

<p><img src="/images/lucene/document_relative_vsm.jpg"></p>

<p>我们认为两个向量之间的夹角越小,相关性越大.所以我们计算夹角的余弦值作为相关性的打分,夹角越小,余弦值越大,打分越高,相关性越大.</p>

<p>需要注意的是,查询语句一般是很短的,包含的词(Term)是很少的,因而查询向量的维数很小,而文档很长,包含词(Term)很多,文档向量维数很大, 但是需要放到相同的向量空间,因此要保证二者维数是相同的.处理方式很简单,维数不同时,取二者的并集,如果不含某个词(Term)时,则权重(Term Weight)为0.</p>

<p>相关性打分公式如下:</p>

<p><img src="/images/lucene/documet_relative_caculate.png"></p>

<p>举个例子,查询语句有11个Term,共有三篇文档搜索出来.其中各自的权重(Term weight),如下:</p>

<p><img src="/images/lucene/document_score.png"></p>

<p>于是计算,三篇文档同查询语句的相关性打分分别为:</p>

<p><img src="/images/lucene/score_example.png"></p>

<p>于是文档二相关性最高,先返回,其次是文档一,最后是文档三.到此为止,我们可以找到我们最想要的文档了.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[分词]]></title>
    <link href="http://xiaobaoqiu.github.io/blog/2015/07/17/fen-ci/"/>
    <updated>2015-07-17T01:24:30+08:00</updated>
    <id>http://xiaobaoqiu.github.io/blog/2015/07/17/fen-ci</id>
    <content type="html"><![CDATA[<p>最近有需求需要使用crate搜索时候的分词功能,正好研究一下搜索中分词相关的基础知识
.</p>

<h1>1.什么是分词</h1>

<h1>2.为什么要分词</h1>

<h1>3.怎么做分词</h1>

<p>参考：<a href="http://my.oschina.net/apdplat/blog/412921">http://my.oschina.net/apdplat/blog/412921</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Crate服务load飙高]]></title>
    <link href="http://xiaobaoqiu.github.io/blog/2015/07/16/cratefu-wu-loadbiao-gao/"/>
    <updated>2015-07-16T05:02:20+08:00</updated>
    <id>http://xiaobaoqiu.github.io/blog/2015/07/16/cratefu-wu-loadbiao-gao</id>
    <content type="html"><![CDATA[<p>前段时间搜索处理个P3的故障,原因是用户输入超级长的搜索字段,导致crate服务器load飙高,进而对外服务失败.</p>

<p>持续时间约10分钟.但是对我而言,最大的问题是这10分钟内,对crate服务器我什么都做不了.经过这次问题,准备好好看看crate以及背后ES的源码.</p>

<p>虽然解决方案是限制用户输入(这一点和google以及百度做法是一样的,百度的查询限制在38个汉字以内).</p>

<h1>1.like查询</h1>

<p>有问题的字段是name字段,使用crate的简单的like查询.根据官方文档,crate的like查询支持两种通配符:</p>

<pre><code>% : 0个或者多个字符
_ : 单个字符
</code></pre>

<p>需要注意的注意,使用like查询可能导致慢查询.特别是当使用前置通配符开头的like查询.因为这种情况下crate需要去迭代所有行,而不能使用索引.
如果向获取更好的性能,可以考虑使用全文索引.</p>

<p>like参考:<a href="https://crate.io/docs/en/latest/sql/queries.html#like">https://crate.io/docs/en/latest/sql/queries.html#like</a></p>

<p>全文索引参考: <a href="https://crate.io/docs/en/latest/sql/fulltext.html">https://crate.io/docs/en/latest/sql/fulltext.html</a> <a href="https://crate.io/docs/en/latest/sql/ddl.html#fulltext-indices">https://crate.io/docs/en/latest/sql/ddl.html#fulltext-indices</a></p>

<h1>2.crate符取异常日志</h1>

<p>这里贴一下当时crate服务器的异常:
<code>
org.elasticsearch.index.query.QueryParsingException: [merchant_sea] Failed to parse
        at org.elasticsearch.index.query.IndexQueryParserService.parseQuery(IndexQueryParserService.java:370)
        at org.elasticsearch.action.count.TransportCountAction.shardOperation(TransportCountAction.java:187)
        at org.elasticsearch.action.count.CrateTransportCountAction.shardOperation(CrateTransportCountAction.java:119)
        at org.elasticsearch.action.count.CrateTransportCountAction.shardOperation(CrateTransportCountAction.java:49)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:171)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.util.automaton.TooComplexToDeterminizeException: Determinizing automaton would result in more than 10000 states.
        at org.apache.lucene.util.automaton.Operations.determinize(Operations.java:743)
        at org.apache.lucene.util.automaton.RunAutomaton.&lt;init&gt;(RunAutomaton.java:138)
        at org.apache.lucene.util.automaton.ByteRunAutomaton.&lt;init&gt;(ByteRunAutomaton.java:32)
        at org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:203)
        at org.apache.lucene.search.AutomatonQuery.&lt;init&gt;(AutomatonQuery.java:84)
        at org.apache.lucene.search.AutomatonQuery.&lt;init&gt;(AutomatonQuery.java:65)
        at org.apache.lucene.search.WildcardQuery.&lt;init&gt;(WildcardQuery.java:57)
        at org.elasticsearch.index.query.WildcardQueryParser.parse(WildcardQueryParser.java:106)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:281)
        at org.elasticsearch.index.query.BoolQueryParser.parse(BoolQueryParser.java:93)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:281)
        at org.elasticsearch.index.query.BoolQueryParser.parse(BoolQueryParser.java:93)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:281)
        at org.elasticsearch.index.query.BoolQueryParser.parse(BoolQueryParser.java:93)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:281)
        at org.elasticsearch.index.query.BoolQueryParser.parse(BoolQueryParser.java:93)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:281)
        at org.elasticsearch.index.query.BoolQueryParser.parse(BoolQueryParser.java:93)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:281)
        at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:382)
        at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:281)
        at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:276)
        at org.elasticsearch.index.query.IndexQueryParserService.parseQuery(IndexQueryParserService.java:354)
        ... 7 more
</code></p>

<h1>3.背后的原理</h1>

<p>查询的第一步是query词的解析，这里使用的like查询，查询类型是通配符的查询(WildcardQuery)。WildcardQuery主要处理三种情况：</p>

<pre><code>  /** 匹配字符串 */
  public static final char WILDCARD_STRING = '*';

  /** 匹配单个字符 */
  public static final char WILDCARD_CHAR = '?';

  /** 转义符 */
  public static final char WILDCARD_ESCAPE = '\\';
</code></pre>

<p>WildcardQuery继承自AutomatonQuery，AutomatonQuery实现了有限状态机匹配的搜索(记得算法导论上有介绍，和KMP算法同时出现的)。</p>

<p>构造AutomatonQuery的时候有一个参数maxDeterminizedStates，意义是构造出来的状态机的最大状态个数。默认值是Operations.DEFAULT_MAX_DETERMINIZED_STATES:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>public AutomatonQuery(final Term term, Automaton automaton) {
</span><span class='line'>    this(term, automaton, Operations.DEFAULT_MAX_DETERMINIZED_STATES);
</span><span class='line'>  }&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;/&lt;em&gt;*
</span><span class='line'>   * Create a new AutomatonQuery from an {@link Automaton}.
</span><span class='line'>   *
</span><span class='line'>   * @param term Term containing field and possibly some pattern structure. The
</span><span class='line'>   *        term text is ignored.
</span><span class='line'>   * @param automaton Automaton to run, terms that are accepted are considered a
</span><span class='line'>   *        match.
</span><span class='line'>   * @param maxDeterminizedStates maximum number of states in the resulting
</span><span class='line'>   *   automata.  If the automata would need more than this many states
</span><span class='line'>   *   TooComplextToDeterminizeException is thrown.  Higher number require more
</span><span class='line'>   *   space but can process more complex automata.
</span><span class='line'>   &lt;/em&gt;/
</span><span class='line'>  public AutomatonQuery(final Term term, Automaton automaton, int maxDeterminizedStates) {
</span><span class='line'>    super(term.field());
</span><span class='line'>    this.term = term;
</span><span class='line'>    this.automaton = automaton;
</span><span class='line'>    this.compiled = new CompiledAutomaton(automaton, null, true, maxDeterminizedStates);
</span><span class='line'>  }</span></code></pre></td></tr></table></div></figure></p>

<p>Operations.DEFAULT_MAX_DETERMINIZED_STATES的真实值为10000</p>

<pre><code>final public class Operations {
  /**
   * Default maximum number of states that {@link Operations#determinize} should create.
   */
  public static final int DEFAULT_MAX_DETERMINIZED_STATES = 10000;
  ...
}
</code></pre>
]]></content>
  </entry>
  
</feed>
