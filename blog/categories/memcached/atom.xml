<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Memcached | xiaobaoqiu Blog]]></title>
  <link href="http://xiaobaoqiu.github.io/blog/categories/memcached/atom.xml" rel="self"/>
  <link href="http://xiaobaoqiu.github.io/"/>
  <updated>2016-07-25T12:36:01+08:00</updated>
  <id>http://xiaobaoqiu.github.io/</id>
  <author>
    <name><![CDATA[xiaobaoqiu]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Memcached存储枚举]]></title>
    <link href="http://xiaobaoqiu.github.io/blog/2015/11/23/memcachedcun-chu-mei-ju/"/>
    <updated>2015-11-23T20:48:42+08:00</updated>
    <id>http://xiaobaoqiu.github.io/blog/2015/11/23/memcachedcun-chu-mei-ju</id>
    <content type="html"><![CDATA[<p>今天发布线上碰到一个小问题，Memcached存储的数据类型包括枚举的时候，当修改枚举的包路径之后，会出现问题。
异常信息如下，比较直观，就是数据的反序列化失败：</p>

<pre><code>[2015-11-23 18:04:45 ERROR net.rubyeye.xmemcached.transcoders.BaseSerializingTranscoder:113] Caught CNFE decoding 944 bytes of data
java.lang.ClassNotFoundException: Xxx.Xxx.Xxx.Xxx.RoomStatus
        at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1702) ~[catalina.jar:7.0.47]
        at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1547) ~[catalina.jar:7.0.47]
        at net.rubyeye.xmemcached.transcoders.BaseSerializingTranscoder$1.resolveClass(BaseSerializingTranscoder.java:102) ~[xmemcached-1.4.1.jar:na]
        at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:1725) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) ~[na:1.7.0_45]
        at java.util.ArrayList.readObject(ArrayList.java:771) ~[na:1.7.0_45]
        at sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source) ~[na:na]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_45]
        at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_45]
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) ~[na:1.7.0_45]
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) ~[na:1.7.0_45]
        at net.rubyeye.xmemcached.transcoders.BaseSerializingTranscoder.deserialize(BaseSerializingTranscoder.java:106) ~[xmemcached-1.4.1.jar:na]
        at net.rubyeye.xmemcached.transcoders.SerializingTranscoder.decode0(SerializingTranscoder.java:92) [xmemcached-1.4.1.jar:na]
        at net.rubyeye.xmemcached.transcoders.SerializingTranscoder.decode(SerializingTranscoder.java:86) [xmemcached-1.4.1.jar:na]
        at net.rubyeye.xmemcached.XMemcachedClient.fetch0(XMemcachedClient.java:630) [xmemcached-1.4.1.jar:na]
        at net.rubyeye.xmemcached.XMemcachedClient.get0(XMemcachedClient.java:1030) [xmemcached-1.4.1.jar:na]
        at net.rubyeye.xmemcached.XMemcachedClient.get(XMemcachedClient.java:988) [xmemcached-1.4.1.jar:na]
        at net.rubyeye.xmemcached.XMemcachedClient.get(XMemcachedClient.java:999) [xmemcached-1.4.1.jar:na]
</code></pre>

<p>场景如下：</p>

<ol>
<li>Memcached已经有数据，数据中包含RoomStatus数据；</li>
<li>修改了枚举的包路径，发布线上；</li>
<li>从Cache中读取之前的数据会出现枚举反序列化异常；</li>
</ol>


<p>在本地代码中重现了一下问题，并看了一下使用枚举的情况下其他可能处心问题的场景，结论如下（前提是Cache中已经存在使用RoomStatus数据）：</p>

<ol>
<li>修改包路径，会导致get数据反序列化失败，java.lang.ClassNotFoundException: Xxx.Xxx.Xxx.Xxx.RoomStatus；</li>
<li>RoomStatus增加项，正常；</li>
<li>RoomStatus减少项(已Cache数据中未使用到的项)，正常；</li>
<li>RoomStatus减少项(已Cache数据中使用到的项)，异常：enum constant Xxx does not exist in Xxx.Xxx.Xxx.Xxx.RoomStatus；</li>
</ol>


<p>综上，在Memcached中cache包含枚举的数据时候，建议如下：</p>

<ol>
<li>不要修改枚举的包路径；</li>
<li>不要删除枚举中的数据；</li>
</ol>


<p>这些序列化和反序列化的逻辑再XMemcached客户端实现的。详见SerializingTranscoder类，包含数据的encode和decode逻辑。再使用Memcached的时候也可以使用</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Memcached安全性]]></title>
    <link href="http://xiaobaoqiu.github.io/blog/2015/11/19/memcachedxian-zhi-fang-wen-ip/"/>
    <updated>2015-11-19T18:39:25+08:00</updated>
    <id>http://xiaobaoqiu.github.io/blog/2015/11/19/memcachedxian-zhi-fang-wen-ip</id>
    <content type="html"><![CDATA[<ul>
<li><a href="#1.Memcached%20-l%E5%8F%82%E6%95%B0">1.Memcached -l参数</a></li>
<li><a href="#2.%E4%BD%BF%E7%94%A8iptable">2.使用iptable</a></li>
<li><a href="#2.%E4%B8%8D%E9%9C%80%E8%A6%81Root%E6%9D%83%E9%99%90">3.不需要Root权限</a></li>
</ul>


<h2 id="1.Memcached -l参数">1.Memcached -l参数</h2>


<p>最近整理了组内使用的Memcached。发现很多问题，其中一个问题就是开发机器测试机器可以直连线上的Memcached。这也是memcached公认的问题：memcached 是一种很简单、有效的协议，但也有其缺点，就是 memcached 自身没有 ACL 控制（或者相当弱）。</p>

<p>Memcache服务器端都是直接通过客户端连接后直接操作，没有任何的验证过程，这样如果服务器是直接暴露在互联网上的话是比较危险，轻则数据泄露被其他无关人员查看，重则服务器被入侵。</p>

<p>乌云也爆料过很多网站的memcached的安全性问题：
<a href="http://www.wooyun.org/bugs/wooyun-2010-0790">http://www.wooyun.org/bugs/wooyun-2010-0790</a>
<a href="http://www.wooyun.org/bugs/wooyun-2013-023891">http://www.wooyun.org/bugs/wooyun-2013-023891</a>
<a href="http://www.wooyun.org/bugs/wooyun-2013-037301">http://www.wooyun.org/bugs/wooyun-2013-037301</a></p>

<p>通过-l参数可以再已定成都上做到安全的限制：</p>

<ol>
<li>如果限定只要自己能够使用本机的Memcached，可以直接将-l参数绑定到回路地址127.0.0.1.</li>
<li>如果是后台系统且有自己的私有IP，最好将-l参数绑定到私有IP上(比如192.168.0.200).</li>
</ol>


<p>如果Memcached非要挂在公网IP上，就需要做防火量限制，如下面说道的iptables。</p>

<h2 id="2.使用iptable">2.使用iptable</h2>


<p>ACL 最简单的设置方法就是在网络层，直接拒绝掉你的访问，通过iptable可以实现这个功能。</p>

<p>假如我们的一台 memcached 的机器，想拒绝除了自身之外的访问，假如机器自己IP是：XX.XX.XX.184，那么我们可以以root身份用下面几条命令来达到我们的目的：</p>

<pre><code>[baoqiu.xiao@... ~]sudo iptables -A INPUT -p tcp -s 127.0.0.1 --dport 6666 -j ACCEPT
[baoqiu.xiao@... ~]sudo iptables -A INPUT -p tcp -s XX.XX.XX.184 --dport 6666 -j ACCEPT
[baoqiu.xiao@... ~]sudo iptables -A INPUT -p tcp --dport 6666 -j REJECT

[baoqiu.xiao@... ~]sudo iptables -L -n --line-number
Chain INPUT (policy ACCEPT)
num  target     prot opt source               destination         
1    ACCEPT     tcp  --  127.0.0.1            0.0.0.0/0           tcp dpt:6666 
2    ACCEPT     tcp  --  XX.XX.XX.184         0.0.0.0/0           tcp dpt:6666 
3    REJECT     tcp  --  0.0.0.0/0            0.0.0.0/0           tcp dpt:6666 reject-with icmp-port-unreachable 

Chain FORWARD (policy ACCEPT)
num  target     prot opt source               destination         

Chain OUTPUT (policy ACCEPT)
num  target     prot opt source               destination
</code></pre>

<p>这里使用的是iptables的filter功能，其filter是一种链式结构且有从前往后依次执行。满足某一条filter规则就不往下走了。因此基于这个原则，我们需要将最严格的规则放在最前面。</p>

<p>删除某一条规则，其中的1就是iptables -L -n &ndash;line-number中的num号。如下会删除Chain INPUT中的编号为1的规则：</p>

<pre><code>[baoqiu.xiao@... ~]sudo iptables -D INPUT 1
</code></pre>

<p>可以使用iptables -F 清空所有规则.</p>

<h2 id="3.不需要Root权限">3.不需要Root权限</h2>


<p>启动Memcached不需要Root权限，这样能避免Memcached被入侵而造成更大的危害。</p>

<p>参考：
<a href="http://blog.couchbase.com/memcached-security">http://blog.couchbase.com/memcached-security</a>
<a href="http://serverfault.com/questions/424324/how-to-secure-memcached">http://serverfault.com/questions/424324/how-to-secure-memcached</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Memcached网络模型]]></title>
    <link href="http://xiaobaoqiu.github.io/blog/2014/11/03/memcachedwang-luo-mo-xing/"/>
    <updated>2014-11-03T18:59:33+08:00</updated>
    <id>http://xiaobaoqiu.github.io/blog/2014/11/03/memcachedwang-luo-mo-xing</id>
    <content type="html"><![CDATA[<p>这篇文章的目的是学习Memcached网络模型相关的源代码.</p>

<p>Memcached采用了很典型的Master-Worker模型，采用的是多线程而不是多进程. 主线程(Master)接收连接, 然后把连接平分派给工作线程(Worker),工作线程处理业务逻辑.</p>

<p>核心的共享数据是消息队列，主线程会把收到的事件请求放入队列，随后调度程序会选择一个空闲的Worker线程来从队列中取出事件请求进行处理.</p>

<h1>1.libevent简介</h1>

<p>Memcached使用libevent实现事件循环，libevent在Linux环境下默认采用epoll作为IO多路复用方法.
用户线程使用libevent则通常按以下步骤：
    (1).用户线程通过event_init()函数创建一个event_base对象。event_base对象管理所有注册到自己内部的IO事件。多线程环境下，event_base对象不能被多个线程共享，即一个event_base对象只能对应一个线程。
    (2).然后该线程通过event_add函数，将与自己感兴趣的文件描述符相关的IO事件，注册到event_base对象，同时指定事件发生时所要调用的事件处理函数（event handler）。服务器程序通常监听套接字（socket）的可读事件。比如，服务器线程注册套接字sock1的EV_READ事件，并指定event_handler1()为该事件的回调函数。libevent将IO事件封装成struct event类型对象，事件类型用EV_READ/EV_WRITE等常量标志。
    (3).注册完事件之后，线程调用event_base_loop进入循环监听（monitor）状态。该循环内部会调用epoll等IO复用函数进入阻塞状态，直到描述符上发生自己感兴趣的事件。此时，线程会调用事先指定的回调函数处理该事件。例如，当套接字sock1发生可读事件，即sock1的内核buff中已有可读数据时，被阻塞的线程立即返回（wake up）并调用event_handler1()函数来处理该次事件。
    (4).处理完这次监听获得的事件后，线程再次进入阻塞状态并监听，直到下次事件发生。</p>

<h1>2.Memcached网络模型</h1>

<p>大致的图示如下:</p>

<p><img src="/images/memcached/memcached-libevent.jpg"></p>

<h3>2.1主要数据结构</h3>

<p>首先是CQ_ITEM, CQ_ITEM实际上是主线程accept后返回的已建立连接的fd的封装:</p>

<pre><code class="c thread.c">/* An item in the connection queue. */
typedef struct conn_queue_item CQ_ITEM;
struct conn_queue_item {
    int               sfd;
    enum conn_states  init_state;
    int               event_flags;
    int               read_buffer_size;
    enum network_transport     transport;
    CQ_ITEM          *next;
};
</code></pre>

<p>CQ是一个管理CQ_ITEM的单向链表:</p>

<pre><code class="c thread.c">/* A connection queue. */
typedef struct conn_queue CQ;
struct conn_queue {
    CQ_ITEM *head;
    CQ_ITEM *tail;
    pthread_mutex_t lock;
};
</code></pre>

<p>LIBEVENT_THREAD是Memcached对线程结构的封装,每个线程都包含一个CQ队列，一条通知管道pipe
和一个libevent的实例event_base :</p>

<pre><code>typedef struct {
    pthread_t thread_id;        /* unique ID of this thread */
    struct event_base *base;    /* libevent handle this thread uses */
    struct event notify_event;  /* listen event for notify pipe */
    int notify_receive_fd;      /* receiving end of notify pipe */
    int notify_send_fd;         /* sending end of notify pipe */
    struct thread_stats stats;  /* Stats generated by this thread */
    struct conn_queue *new_conn_queue; /* CQ队列 */
    cache_t *suffix_cache;      /* suffix cache */
    uint8_t item_lock_type;     /* use fine-grained or global item lock */
} LIBEVENT_THREAD;
</code></pre>

<h3>2.2主流程</h3>

<p>在memcached.c的main函数中展示了客户端请求处理的主流程:</p>

<p><strong>(1).对主线程的libevent做了初始化</strong></p>

<pre><code class="c">/* initialize main thread libevent instance */  
 main_base = event_init();
</code></pre>

<p><strong>(2).初始化所有的线程(包括Master和Worker线程),并启动</strong></p>

<pre><code class="c">/* start up worker threads if MT mode */  
thread_init(settings.num_threads, main_base);
</code></pre>

<p>其中settings.num_threads表示线程数目,默认是4个:</p>

<pre><code class="c">settings.num_threads = 4;         /* N workers */
</code></pre>

<p>下面简单分析thread_init的核心代码(thread.c):</p>

<pre><code class="c">/*
 * Initializes the thread subsystem, creating various worker threads.
 *
 * nthreads  Number of worker event handler threads to spawn
 * main_base Event base for main thread
 */
void thread_init(int nthreads, struct event_base *main_base) {

    ...//省略若干代码

    //threads的声明在thread.c头部,用于保存所有的线程
    threads = calloc(nthreads, sizeof(LIBEVENT_THREAD));
    if (! threads) {
        perror("Can't allocate thread descriptors");
        exit(1);
    }

    dispatcher_thread.base = main_base;
    dispatcher_thread.thread_id = pthread_self();

    for (i = 0; i &lt; nthreads; i++) {
        int fds[2];
        if (pipe(fds)) {    //创建管道
            perror("Can't create notify pipe");
            exit(1);
        }

        threads[i].notify_receive_fd = fds[0];  //读端
        threads[i].notify_send_fd = fds[1];     //写端

        //创建所有workers线程的libevent实例
        setup_thread(&amp;threads[i]);
        /* Reserve three fds for the libevent base, and two for the pipe */
        stats.reserved_fds += 5;
    }

    //创建线程
    /* Create threads after we've done all the libevent setup. */
    for (i = 0; i &lt; nthreads; i++) {
        create_worker(worker_libevent, &amp;threads[i]);
    }

    //等待所有线程启动起来之后,这个函数再返回
    /* Wait for all the threads to set themselves up before returning. */
    pthread_mutex_lock(&amp;init_lock);
    wait_for_thread_registration(nthreads);
    pthread_mutex_unlock(&amp;init_lock);
}
</code></pre>

<p>thread_init首先malloc线程的空间，然后第一个threads作为主线程，其余都是workers线程
然后为每个线程创建一个pipe，这个pipe被用来作为主线程通知workers线程有新的连接到达.</p>

<p>其中pipe()函数用于创建管道,管道两端可分别用描述字fds[0]以及fds[1]来描述.需要注意的是，管道的两端是固定的。即一端只能用于读，由描述字fds[0]表示，称其为管道读端；另一端则只能用于写，由描述字fds[1]来表示，称其为管道写端.</p>

<p>setup_thread主要是创建所有workers线程的libevent实例（主线程的libevent实例在main函数中已经建立）,setup_thread()的代码如下:</p>

<pre><code class="c">/*
 * Set up a thread's information.
 */
static void setup_thread(LIBEVENT_THREAD *me) {
    me-&gt;base = event_init();
    if (! me-&gt;base) {
        fprintf(stderr, "Can't allocate event base\n");
        exit(1);
    }

    //注意这里只有notify_receive_fd,即读端口
    /* Listen for notifications from other threads */
    event_set(&amp;me-&gt;notify_event, me-&gt;notify_receive_fd,
              EV_READ | EV_PERSIST, thread_libevent_process, me);
    event_base_set(me-&gt;base, &amp;me-&gt;notify_event);

    if (event_add(&amp;me-&gt;notify_event, 0) == -1) {
        fprintf(stderr, "Can't monitor libevent notify pipe\n");
        exit(1);
    }

    me-&gt;new_conn_queue = malloc(sizeof(struct conn_queue));
    if (me-&gt;new_conn_queue == NULL) {
        perror("Failed to allocate memory for connection queue");
        exit(EXIT_FAILURE);
    }
    cq_init(me-&gt;new_conn_queue);

    ...
}
</code></pre>

<p>这里会为所有worker thread线程注册与notify_event_fd描述符有关的IO事件，这里的notify_event_fd描述符是该worker thread线程与main thread线程通信的管道的接收端(读)描述符。通过注册与该描述符有关的IO事件，worker thread线程就能监听main thread线程发给自己的数据(即事件).</p>

<p>注意这里event_set中的thread_libevent_process参数,其意义在于监听Worker线程与main thread线程通信的管道上的可读事件，并指定用thread_libevent_process()函数处理该事件,即每次管道读端有数据刻度,即触发thread_libevent_process过程.</p>

<p>thread_libevent_process的代码如下,其中最重要的一个就是数据为c的,后续会详细分析这块代码.</p>

<pre><code class="c">/*
 * Processes an incoming "handle a new connection" item. This is called when
 * input arrives on the libevent wakeup pipe.
 */
static void thread_libevent_process(int fd, short which, void *arg) {
    LIBEVENT_THREAD *me = arg;
    CQ_ITEM *item;
    char buf[1];

    //从管道中读数据
    if (read(fd, buf, 1) != 1)
        if (settings.verbose &gt; 0)
            fprintf(stderr, "Can't read from libevent pipe\n");

    switch (buf[0]) {
    case 'c':   //c表示有新的连接请求被主线程分配到当前Worker线程
    item = cq_pop(me-&gt;new_conn_queue);

    if (NULL != item) {
        conn *c = conn_new(item-&gt;sfd, item-&gt;init_state, item-&gt;event_flags,
                           item-&gt;read_buffer_size, item-&gt;transport, me-&gt;base);
        if (c == NULL) {
            if (IS_UDP(item-&gt;transport)) {
                fprintf(stderr, "Can't listen for events on UDP socket\n");
                exit(1);
            } else {
                if (settings.verbose &gt; 0) {
                    fprintf(stderr, "Can't listen for events on fd %d\n",
                        item-&gt;sfd);
                }
                close(item-&gt;sfd);
            }
        } else {
            c-&gt;thread = me;
        }
        cqi_free(item);
    }
        break;
    /* we were told to flip the lock type and report in */
    case 'l':
        ...
    case 'g':
        ...
    }
}
</code></pre>

<p>thread_init函数中create_worker实际上就是真正启动了线程, create_worker的代码如下:</p>

<pre><code class="c">/*
 * Creates a worker thread.
 */
static void create_worker(void *(*func)(void *), void *arg) {
    pthread_t       thread;
    pthread_attr_t  attr;
    int             ret;

    pthread_attr_init(&amp;attr);

    if ((ret = pthread_create(&amp;thread, &amp;attr, func, arg)) != 0) {
        fprintf(stderr, "Can't create thread: %s\n",
                strerror(ret));
        exit(1);
    }
}
</code></pre>

<p>pthread_create是创建线程函数,第三个参数是线程运行函数的起始地址,这里即worker_libevent函数,该方法执行event_base_loop启动该线程的libevent.</p>

<pre><code class="c">/*
 * Worker thread: main event loop
 */
static void *worker_libevent(void *arg) {
    LIBEVENT_THREAD *me = arg;

    /* Any per-thread setup can happen here; thread_init() will block until
     * all threads have finished initializing.
     */

    /* set an indexable thread-specific memory item for the lock type.
     * this could be unnecessary if we pass the conn *c struct through
     * all item_lock calls...
     */
    me-&gt;item_lock_type = ITEM_LOCK_GRANULAR;
    pthread_setspecific(item_lock_type_key, &amp;me-&gt;item_lock_type);

    register_thread_initialized();

    event_base_loop(me-&gt;base, 0);
    return NULL;
}
</code></pre>

<p>这里我们需要记住每个workers线程目前只在自己线程的管道的读端有数据时可读时触发，并调用
thread_libevent_process方法.</p>

<p><strong>(3).主线程调用</strong></p>

<pre><code class="c">/* create the listening socket, bind it, and init */
server_sockets(settings.port, tcp_transport, portnumber_file) 
</code></pre>

<p>在worker thread线程启动后，main thread线程就要创建监听套接字（listening socket）来等待客户端连接请求。这个方法主要是封装了创建监听socket，绑定地址，设置非阻塞模式并注册监听socket的libevent 读事件等一系列操作.</p>

<p>套接字被封装成conn对象，表示与客户端的连接,定义十分庞大(见memcached.h).</p>

<p>端口号默认是11211:</p>

<pre><code class="c">settings.port = 11211;
</code></pre>

<p>server_sockets函数主要调用server_socket()函数:</p>

<pre><code class="c">/**
 * Create a socket and bind it to a specific port number
 * @param interface the interface to bind to
 * @param port the port number to bind to
 * @param transport the transport protocol (TCP / UDP)
 * @param portnumber_file A filepointer to write the port numbers to
 *        when they are successfully added to the list of ports we
 *        listen on.
 */
static int server_socket(const char *interface,
                         int port,
                         enum network_transport transport,
                         FILE *portnumber_file) {

    ...//省略若干代码

    //主机名到地址解析,结果存在ai中,为addrinfo的链表
    error= getaddrinfo(interface, port_buf, &amp;hints, &amp;ai);
    if (error != 0) {
        if (error != EAI_SYSTEM)
          fprintf(stderr, "getaddrinfo(): %s\n", gai_strerror(error));
        else
          perror("getaddrinfo()");
        return 1;
    }

    for (next= ai; next; next= next-&gt;ai_next) {
        conn *listen_conn_add;
        if ((sfd = new_socket(next)) == -1) {   //创建socket
            /* getaddrinfo can return "junk" addresses,
             * we make sure at least one works before erroring.
             */
            if (errno == EMFILE) {
                /* ...unless we're out of fds */
                perror("server_socket");
                exit(EX_OSERR);
            }
            continue;
        }
        //IPV4地址,设置socket选项
        setsockopt(sfd, SOL_SOCKET, SO_REUSEADDR, (void *)&amp;flags, sizeof(flags));
        ...//省略若干代码

        //socket和地址绑定
        if (bind(sfd, next-&gt;ai_addr, next-&gt;ai_addrlen) == -1) {
            if (errno != EADDRINUSE) {
                perror("bind()");
                close(sfd);
                freeaddrinfo(ai);
                return 1;
            }
            close(sfd);
            continue;
        } else {
            success++;
            ...//省略若干代码
        }

        if (IS_UDP(transport)) {
            ...//省略若干代码
        } else {
            if (!(listen_conn_add = conn_new(sfd, conn_listening,
                                             EV_READ | EV_PERSIST, 1,
                                             transport, main_base))) {
                fprintf(stderr, "failed to create listening connection\n");
                exit(EXIT_FAILURE);
            }
            listen_conn_add-&gt;next = listen_conn;
            listen_conn = listen_conn_add;
        }
    }

    freeaddrinfo(ai);

    /* Return zero iff we detected no errors in starting up connections */
    return success == 0;
}
</code></pre>

<p>conn_new()是这里的最关键的一个函数,此函数负责将原始套接字封装成为一个conn对象，同时会注册与该conn对象相关的IO事件，并指定该连接（conn）的初始状态。这里要注意的是listening socket的conn对象被初始化为conn_listening状态.</p>

<pre><code class="c">conn *conn_new(const int sfd, enum conn_states init_state,
                const int event_flags,
                const int read_buffer_size, enum network_transport transport,
                struct event_base *base) {

    ...//省略若干代码

    //设置fd和初始状态
    c-&gt;sfd = sfd;                
    c-&gt;state = init_state;

    //注册与该连接有关的IO事件
    event_set(&amp;c-&gt;event, sfd, event_flags, event_handler, (void *)c);
    event_base_set(base, &amp;c-&gt;event);
    c-&gt;ev_flags = event_flags;

    if (event_add(&amp;c-&gt;event, 0) == -1) {
        perror("event_add");
        return NULL;
    }

    ...//
}
</code></pre>

<p>所有conn对象IO事件相关的处理函数都是event_handler()函数,这个函数主要是调用drive_machine()函数:</p>

<pre><code class="c">void event_handler(const int fd, const short which, void *arg) {
    conn *c;
    ...
    drive_machine(c);
    ...
}
</code></pre>

<p>drive_machine这个函数就全权负责处理与客户连接相关的事件:</p>

<pre><code class="c">static void drive_machine(conn *c) {
    ...
    assert(c != NULL);

    while (!stop) {
        switch(c-&gt;state) {
        case conn_listening:
            addrlen = sizeof(addr);
            sfd = accept(c-&gt;sfd, (struct sockaddr *)&amp;addr, &amp;addrlen);
            ...

            if (settings.maxconns_fast &amp;&amp;
                ...
            } else {
                dispatch_conn_new(sfd, conn_new_cmd, EV_READ | EV_PERSIST,
                                     DATA_BUFFER_SIZE, tcp_transport);
            }

            stop = true;
            break;
        case conn_waiting:
        ...
        case conn_read:
        ...
        case conn_parse_cmd:
        ...
        case conn_new_cmd:
        ...
        case conn_nread:
        ...
        case conn_swallow:
        ...
        case conn_write:
        ...
        case conn_mwrite:
        ...
        case conn_closing:
        ...
        case conn_closed:
    ...
}
</code></pre>

<p>drive_machine中就是conn对象的state字段发挥作用的地方了,drive_machine()函数是一个巨大的switch语句，它根据conn对象的当前状态，即state字段的值选择执行不同的分支，因为listening socket的conn对象被初始化为conn_listening状态，所以drive_machine()函数会执行switch语句中case conn_listenning的分支，即接受客户端连接并通过dispatch_conn_new()函数将连接分派给Worker线程.</p>

<p>dispatch_conn_new代码如下(thread.c):</p>

<pre><code class="c">/*
 * Dispatches a new connection to another thread. This is only ever called
 * from the main thread, either during initialization (for UDP) or because
 * of an incoming connection.
 */
void dispatch_conn_new(int sfd, enum conn_states init_state, int event_flags,
                       int read_buffer_size, enum network_transport transport) {
    CQ_ITEM *item = cqi_new();  //新申请一个CQ_ITEM
    char buf[1];
    if (item == NULL) {
        close(sfd);
        /* given that malloc failed this may also fail, but let's try */
        fprintf(stderr, "Failed to allocate memory for connection object\n");
        return ;
    }

    //分发给Worker线程
    int tid = (last_thread + 1) % settings.num_threads;

    LIBEVENT_THREAD *thread = threads + tid;

    last_thread = tid;

    item-&gt;sfd = sfd;
    item-&gt;init_state = init_state;  //注意这里的状态为conn_new_cmd
    item-&gt;event_flags = event_flags;
    item-&gt;read_buffer_size = read_buffer_size;
    item-&gt;transport = transport;

    //把新申请的CQ_ITEM放到被分配的Worker线程的队列中
    cq_push(thread-&gt;new_conn_queue, item);

    MEMCACHED_CONN_DISPATCH(sfd, thread-&gt;thread_id);
    buf[0] = 'c';
    //向worker thread线程的管道写入一字节的数据
    if (write(thread-&gt;notify_send_fd, buf, 1) != 1) {
        perror("Writing to thread notify pipe");
    }
}
</code></pre>

<p>向Worker线程写一个字符的意义在于触发Worker线程管道的读端，即notify_receive_fd描述符的可读事件.</p>

<p>主线程在新连接到来的时候是如何选择处理副线程的呢?很简单,有一个计数器last_thread, 每次将last_thread加一,再模线程数来选择线程ID.</p>

<p>通过之前的分析,我们知道,Worker线程的管道有读时间触发的时候,会调用thread_libevent_process来处理,这里详细分析一下:</p>

<pre><code class="c">/*
 * Processes an incoming "handle a new connection" item. This is called when
 * input arrives on the libevent wakeup pipe.
 */
static void thread_libevent_process(int fd, short which, void *arg) {
    LIBEVENT_THREAD *me = arg;
    CQ_ITEM *item;
    char buf[1];

    //从管道中读数据
    if (read(fd, buf, 1) != 1)
        if (settings.verbose &gt; 0)
            fprintf(stderr, "Can't read from libevent pipe\n");

    switch (buf[0]) {
    case 'c':   //c表示有新的连接请求被主线程分配到当前Worker线程
    //从当前Worker线程的连接请求队列中弹出一个请求
    //此对象即先前main thread线程推入new_conn_queue队列的对象
    item = cq_pop(me-&gt;new_conn_queue);

    if (NULL != item) {
        //根据这个CQ_ITEM对象，创建并初始化conn对象
        //该对象负责客户端与该worker thread线程之间的通信
        conn *c = conn_new(item-&gt;sfd, item-&gt;init_state, item-&gt;event_flags,
                           item-&gt;read_buffer_size, item-&gt;transport, me-&gt;base);
        if (c == NULL) {
            if (IS_UDP(item-&gt;transport)) {
                fprintf(stderr, "Can't listen for events on UDP socket\n");
                exit(1);
            } else {
                if (settings.verbose &gt; 0) {
                    fprintf(stderr, "Can't listen for events on fd %d\n",
                        item-&gt;sfd);
                }
                close(item-&gt;sfd);
            }
        } else {
            c-&gt;thread = me;
        }
        cqi_free(item);
    }
        break;
    /* we were told to flip the lock type and report in */
    case 'l':
        ...
    case 'g':
        ...
    }
}
</code></pre>

<p>到这里,Worker线程就建立了和客户端的连接.</p>

<p>conn_new的一个值得注意的地方就是会设置线程的事件处理函数:</p>

<pre><code class="c">conn *conn_new(const int sfd, enum conn_states init_state,
                const int event_flags,
                const int read_buffer_size, enum network_transport transport,
                struct event_base *base) {
    ...
    event_set(&amp;c-&gt;event, sfd, event_flags, event_handler, (void *)c);
        event_base_set(base, &amp;c-&gt;event);
        c-&gt;ev_flags = event_flags;

        if (event_add(&amp;c-&gt;event, 0) == -1) {
            perror("event_add");
            return NULL;
        }
    ...
}
</code></pre>

<p>我们可以看到,Worker线程也是使用event_handler函数来处理客户端请求过来的数据,根当前请求连接的状态来处理.</p>

<p><strong>(4).事件循环</strong></p>

<pre><code class="c">/* enter the event loop */  
event_base_loop(main_base, 0);
</code></pre>

<p>这时主线程启动开始通过libevent来接受外部连接请求，整个启动过程完毕.</p>

<h1>3.总结</h1>

<p>Memcached中采用的就是所谓的半同步-半异步模式,最早应该是由ACE的作者提出,原文在<a href="http://www.cs.wustl.edu/~schmidt/PDF/HS-HA.pdf">这里</a>.</p>

<p>简单示意图如下:</p>

<p><img src="/images/memcached/half-sync-half-async.jpg"></p>

<h3>3.1半同步-半异步模式</h3>

<p>几个模块的之间的交互为:</p>

<pre><code>(1).异步模块接收可能会异步到来的各种事件(I/O,信号等),然后将它们放入队列中;
(2).同步模块一般只有一种动作,就是不停的从队列中取出消息进行处理;
</code></pre>

<p>半同步-半异步模式的出现是为了给服务器的功能进行划分,尽可能将的可能阻塞的操作放在同步模块中,这样不会影响到异步模块的处理.</p>

<p>举个例子说明:</p>

<p>假设现在有一个服务器,在接收完客户端请求之后会去数据库查询,这个查询可能会很慢.这时,如果还是采用的把接收客户端的连接和处理客户端的请求(在这里这个处理就是查询数据库)放在一个模块中来处理,很可能将会有很多连接的处理响应非常慢.</p>

<p>此时,考虑使用半同步半异步的模式,开一个进程,使用多路复用IO(如epoll/select)等监听客户端的连接,接收到新的连接请求之后就将这些请求存放到通过某种IPC方式实现的消息队列中,同时,还有N个处理进程,它们所做的工作就是不停的从消息队列中取出消息进行处理.这样的划分,将接收客户端请求和处理客户端请求划分为不同的模块,相互之间的通过IPC进行通讯,将对彼此功能的影响限制到最小.</p>

<p><strong>优点</strong></p>

<pre><code>(1).接收操作只在主循环中处理,因此不会出现惊群现象;
(2).主副线程分工明确, 主线程仅负责I/O, 副线程负责业务逻辑处理;
(3).多个副线程之间不会有影响,因为大家都有各自独立的连接队列;
</code></pre>

<p><strong>缺点</strong></p>

<p>假如业务逻辑是类似于web服务器之类的, 那么一个简单的请求也需要这个比较繁琐的操作的话(最重要的是,很可能一个进程就能处理完的事情,非得从一个线程接收再到另一个线程去处理), 那么显然代价是不值得的.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[XMemcached Client]]></title>
    <link href="http://xiaobaoqiu.github.io/blog/2014/10/16/xmemcached-client/"/>
    <updated>2014-10-16T22:51:01+08:00</updated>
    <id>http://xiaobaoqiu.github.io/blog/2014/10/16/xmemcached-client</id>
    <content type="html"><![CDATA[<h1>1.XMemcached是什么</h1>

<p>XMemcached是众多Memcached Client中的后起之秀,而且还是一个Chinese主导的.写这篇Blog的目的就是研究下这个Memcached Client的代码,顺便研究下传说中的一致性Hash.</p>

<p>XMemcached项目:<a href="https://code.google.com/p/xmemcached/">https://code.google.com/p/xmemcached/</a></p>

<p>源代码:<a href="https://github.com/killme2008/xmemcached">https://github.com/killme2008/xmemcached</a></p>

<p>使用文档:<a href="http://blog.sina.com.cn/s/blog_6094008a0102v6gj.html">http://blog.sina.com.cn/s/blog_6094008a0102v6gj.html</a></p>

<p>特点:</p>

<pre><code>(1).支持所有的Memcached文本协议(text based protocols)和二进制协议(二进制协议从1.2.0开始支持);
(2).支持分布式的Memcached,包括标准Hash和一致性哈希策略;
(3).支持JMX,从而允许使用者监控和控制XMemcached Client的行为;同时可以修改优化参数,动态添加或者删除服务器;
(4).支持待权重的服务器配置;
(5).支持连接池,使用Java的nio,对同一个Memcached服务器使用者能够创建更多的连接;
(6).支持故障模式和备用节点;
(7).支持和Spring框架和hibernate-memcached的整合;
(8).高性能;
(9).支持和kestrel(一个Scala实现的MQ)和TokyoTyrant的对话;
</code></pre>

<h1>2.XMemcached的类图</h1>

<p>XMemcached的类图(几个主要的类):</p>

<p><img src="/images/xmemcached/xmemcached-classes.jpg"></p>

<h1>3.XMemcached的示例</h1>

<p>源代码中自带了例子,我们分析一个SimpleExample.java:</p>

<pre><code class="java">public class SimpleExample {
    public static void main(String[] args) {
        if (args.length &lt; 1) {
            System.err.println("Useage:java SimpleExample [servers]");
            System.exit(1);
        }
        MemcachedClient memcachedClient = getMemcachedClient(args[0]);
        if (memcachedClient == null) {
            throw new NullPointerException("Null MemcachedClient,please check memcached has been started");
        }
        try {
            // add a,b,c
            System.out.println("Add a,b,c");
            memcachedClient.set("a", 0, "Hello,xmemcached");
            memcachedClient.set("b", 0, "Hello,xmemcached");
            memcachedClient.set("c", 0, "Hello,xmemcached");
            // get a
            String value = memcachedClient.get("a");
            System.out.println("get a=" + value);
            System.out.println("delete a");
            // delete a
            memcachedClient.delete("a");
            // reget a
            value = memcachedClient.get("a");
            System.out.println("after delete,a=" + value);

            System.out.println("Iterate all keys...");
            // iterate all keys
            KeyIterator it = memcachedClient.getKeyIterator(AddrUtil.getOneAddress(args[0]));
            while (it.hasNext()) {
                System.out.println(it.next());
            }
            System.out.println(memcachedClient.touch("b", 1000));

        } catch (MemcachedException e) {
            System.err.println("MemcachedClient operation fail");
            e.printStackTrace();
        } catch (TimeoutException e) {
            System.err.println("MemcachedClient operation timeout");
            e.printStackTrace();
        } catch (InterruptedException e) {
            // ignore
        }
        try {
            memcachedClient.shutdown();
        } catch (Exception e) {
            System.err.println("Shutdown MemcachedClient fail");
            e.printStackTrace();
        }
    }

    public static MemcachedClient getMemcachedClient(String servers) {
        try {
            //默认是Text协议
            MemcachedClientBuilder builder = new XMemcachedClientBuilder(AddrUtil.getAddresses(servers));
            return builder.build();
        } catch (IOException e) {
            System.err.println("Create MemcachedClient fail");
            e.printStackTrace();
        }
        return null;
    }
}
</code></pre>

<p>本地先起来两个Memcached实例:</p>

<pre><code>/usr/bin/memcached -m 256 -p 11211 -u memcache -l 127.0.0.1 -d
/usr/bin/memcached -m 256 -p 11222 -u memcache -l 127.0.0.1 -d
</code></pre>

<p>在IDEA下运行,配上运行参数,即Memcached服务器的IP和端口,格式如下(解析代码见AddrUtil.java):</p>

<pre><code>127.0.0.1:11211 127.0.0.1:11222
</code></pre>

<p>Debug代码,可以发现其运行过程,下面是get的一个运行序列图:</p>

<p><img src="/images/xmemcached/xmemcached-get.jpg"></p>

<h1>4.XMemcached的分布式实现</h1>

<p>memcached虽然称为“分布式”缓存服务器,但服务器端并没有"&ldquo;分布式”功能.至于memcached的分布式,则是完全由客户端程序库实现的.</p>

<h3>4.1 Memcached的分布式是什么意思</h3>

<p>这里多次使用了“分布式”这个词,但并未做详细解释.现在开始简单地介绍一下其原理,各个客户端的实现基本相同.</p>

<p>下面假设memcached服务器有node1～node3三台, 应用程序要保存键名为“tokyo”“kanagawa”“chiba”“saitama”“gunma” 的数据.</p>

<p><img src="/images/memcached/memcached-distribute-1.png"></p>

<p>首先向memcached中添加“tokyo”.将“tokyo”传给客户端程序库后, 客户端实现的算法就会根据“键”来决定保存数据的memcached服务器. 服务器选定后,即命令它保存“tokyo”及其值.</p>

<p><img src="/images/memcached/memcached-distribute-2.png"></p>

<p>同样,“kanagawa”“chiba”“saitama”“gunma”都是先选择服务器再保存.</p>

<p>接下来获取保存的数据.获取时也要将要获取的键“tokyo”传递给函数库. 函数库通过与数据保存时相同的算法,根据“键”选择服务器. 使用的算法相同,就能选中与保存时相同的服务器,然后发送get命令. 只要数据没有因为某些原因被删除,就能获得保存的值.</p>

<p><img src="/images/memcached/memcached-distribute-3.png"></p>

<p>这样,将不同的键保存到不同的服务器上,就实现了memcached的分布式. memcached服务器增多后,键就会分散,即使一台memcached服务器发生故障
无法连接,也不会影响其他的缓存,系统依然能继续运行.</p>

<h3>4.2 余数哈希</h3>

<p>XMemcached默认使用的是Native Hash,见ArrayMemcachedSessionLocator.java:</p>

<pre><code class="java">public final long getHash(int size, String key) {
    long hash = this.hashAlgorighm.hash(key);
    return hash % size;
}
</code></pre>

<p>ArrayMemcachedSessionLocator中使用的hashAlgorighm是HashAlgorithm.NATIVE_HASH,而HashAlgorithm.NATIVE_HASH的hash()函数的实现:</p>

<pre><code class="java">public long hash(final String k) {
    long rv = 0;
    switch (this) {
    case NATIVE_HASH:
        rv = k.hashCode();
        break;
    ...
</code></pre>

<p>即XMemcached的Native Hash就是通常的余数哈希:首先求得字符串的hash值,根据该值除以服务器节点数目得到的余数决定服务器.</p>

<p>根据这种hash方式,上门这几个key分布的服务器如下(三台服务器分别为 0 1 2):</p>

<pre><code class="java">tokyo --&gt; 2
kanagawa --&gt; 1
chiba --&gt; 2
saitama --&gt; 1
gunma --&gt; 2
</code></pre>

<p><strong>优点</strong>
简单高效:计算简单,数据的分散性也相当优秀</p>

<p><strong>缺点</strong>
当添加或移除服务器时,缓存重组的代价相当巨大. 添加服务器后,余数就会产生巨变,这样就无法获取与保存时相同的服务器, 从而影响缓存的命中率.</p>

<p>下面简单验证这个:首先在3台服务器的情况下将“a”到“z”的键保存到memcached并访问的情况;接下来增加一台memcached服务器;计算缓存命中率;</p>

<p>简单测试代码(忍不住吐槽下java真是一门罗嗦的语言):</p>

<pre><code class="java">public class Hash {

    /**
     * 默认的Hash算法
     */
    private static HashAlgorithm hashAlgorithm = HashAlgorithm.NATIVE_HASH;

    /**
     * Memcached服务器数目
     */
    private static final int SERVER_SIZE = 3;

    public static void main(String[] args) {
        // 初始化
        Map&lt;Long, List&lt;String&gt;&gt; itemMap = new HashMap&lt;Long, List&lt;String&gt;&gt;();
        for (long i = 0; i &lt; SERVER_SIZE; i++) {
            itemMap.put(i, new LinkedList&lt;String&gt;());
        }

        char letters[] = new char[27];
        letters[0] = 'a';
        for (int i = 0; i &lt; 26; i++) {
            letters[i + 1] = (char) (letters[i] + 1);
            long hash = getHash(SERVER_SIZE, String.valueOf(letters[i]));
            itemMap.get(hash).add(String.valueOf(letters[i]));
        }

        for (Map.Entry&lt;Long, List&lt;String&gt;&gt; e : itemMap.entrySet()) {
            System.out.println(e.getKey() + " : " + join(e.getValue()));
        }

    }

    public static long getHash(int size, String key) {
        long hash = hashAlgorithm.hash(key);
        return hash % size;
    }

    private static String join(final List&lt;String&gt; list) {
        StringBuilder builder = new StringBuilder();
        for(String s : list) {
            builder.append(s + ' ');
        }

        return builder.toString();
    }
}
</code></pre>

<p>我们得到3台服务器(分别为0,1,2)的时候,26个字母的分布如下:</p>

<pre><code>0 : c f i l o r u x 
1 : a d g j m p s v y 
2 : b e h k n q t w z 
</code></pre>

<p>4台服务器(分别为0,1,2,3)的时候,26个字母的分布如下:</p>

<pre><code>0 : d h l p t x 
1 : a e i m q u y 
2 : b f j n r v z 
3 : c g k o s w
</code></pre>

<p>根据这两份数据,我们可以得到,加拉一台服务器之后,26个字母只有8个命中了.像这样,添加节点后键分散到的服务器会发生巨大变化.</p>

<p>同样,减少服务器(比如某一台服务器down机),也会导致大量的Miss,基本失去了缓存的作用.</p>

<p>带来的问题就是,在Web应用程序中使用memcached时, 在添加memcached服务器的瞬间缓存效率会大幅度下降,有可能会发生无法提供正常服务的情况.</p>

<h3>4.3 一致性哈希(Consistent Hashing)</h3>

<p>一致性哈希能够很大程度上(注意不是完全解决)解决余数哈希的增加服务器导致缓存失效的问题.</p>

<p><strong>4.3.1 一致性哈希原理</strong>
Consistent Hashing如下所示：首先求出memcached服务器（节点）的哈希值,
并将其配置到0～2SUP(32)的圆（continuum）上. 然后用同样的方法求出存储数据的键的哈希值,并映射到圆上. 然后从数据映射到的位置开始顺时针查找,将数据保存到找到的第一个服务器上. 如果超过2SUP(32)仍然找不到服务器,就会保存到第一台memcached服务器上.</p>

<p><img src="/images/memcached/memcached-consistent-hash1.png"></p>

<p>从上图的状态中添加一台memcached服务器.余数分布式算法由于保存键的服务器会发生巨大变化 而影响缓存的命中率,但Consistent Hashing中,只有在continuum上增加服务器的地点逆时针方向的 第一台服务器上的键会受到影响.</p>

<p><img src="/images/memcached/memcached-consistent-hash2.png"></p>

<p>因此,Consistent Hashing最大限度地抑制了键的重新分布. 而且,有的Consistent Hashing的实现方法还采用了虚拟节点的思想. 使用一般的hash函数的话,服务器的映射地点的分布非常不均匀. 因此,使用虚拟节点的思想,为每个物理节点（服务器） 在continuum上分配100～200个点.这样就能抑制分布不均匀, 最大限度地减小服务器增减时的缓存重新分布.</p>

<p><strong>4.3.2 虚拟节点</strong>
 一致性哈希算法在服务节点太少时,容易因为节点分部不均匀而造成数据倾斜问题.例如我们的系统中有两台 server,其环分布如下：</p>

<p> <img src="/images/xmemcached/virtual-node-why.png"></p>

<p> 此时必然造成大量数据集中到Server 1上,而只有极少量会定位到Server 2上.为了解决这种数据倾斜问题,一致性哈希算法引入了虚拟节点机制.</p>

<p>虚拟节点(virtual node)是实际节点(服务器)在 hash 空间的复制品,一个实际节点(服务器)对应了若干个虚拟节点,这个对应个数也称为为复制个数,虚拟节点在 hash 环中以hash值排列.</p>

<p>例如为上面的每台服务器设置三个虚拟节点:</p>

<p><img src="/images/xmemcached/virtual-node.png"></p>

<p>在实际应用中,一个物理节点对应多少的虚拟节点才能达到比较好的均衡效果,有一个效果图:</p>

<p><img src="/images/xmemcached/virtual-node-count.png"></p>

<p>纵轴为物理服务器的数目,横轴为虚拟节点的数目,可以看出,当物理服务器的数量很小时,需要更大的虚拟节点,反之则需要更少的节点,从图上可以看出,在物理服务器有10台时,差不多需要为每台服务器增加100~200个虚拟节点效果比较好.</p>

<h3>4.4 XMemcached一致性哈希的实现</h3>

<p>XMemcached Client中实现了一致性哈希,见KetamaMemcachedSessionLocator.java,使用的Hash算法是KETAMA HASH算法.</p>

<p>下面主要分析一下KetamaMemcachedSessionLocator.java代码,去掉了一些无关的代码:</p>

<p><strong>4.4.1 根据服务器列表生成Hash环</strong></p>

<pre><code class="java">    private final void buildMap(Collection&lt;Session&gt; list, HashAlgorithm alg) {
        TreeMap&lt;Long, List&lt;Session&gt;&gt; sessionMap = new TreeMap&lt;Long, List&lt;Session&gt;&gt;();

        for (Session session : list) {  //遍历每个服务器
            String sockStr = null;  //根据服务器地址得到的字符串,用于hash
            if (this.cwNginxUpstreamConsistent) {
                ... //生成sockStr
            } else {
                ... //生成sockStr
            }
            /**
             * Duplicate 160 X weight references
             */
            int numReps = NUM_REPS; //虚拟节点数目, 默认160
            //考虑权重,权重小的服务器最终生成的虚拟节点就少,数据打到这个服务器的概率就小
            if (session instanceof MemcachedTCPSession) {
                numReps *= ((MemcachedSession) session).getWeight();
            }
            if (alg == HashAlgorithm.KETAMA_HASH) {     //一致性Hash
                for (int i = 0; i &lt; numReps / 4; i++) {
                    //计算Hash值,将虚拟节点映射到当前session代表的服务器
                    byte[] digest = HashAlgorithm.computeMd5(sockStr + "-" + i);
                    for (int h = 0; h &lt; 4; h++) {
                        long k = (long) (digest[3 + h * 4] &amp; 0xFF) &lt;&lt; 24 | (long) (digest[2 + h * 4] &amp; 0xFF) &lt;&lt; 16
                                | (long) (digest[1 + h * 4] &amp; 0xFF) &lt;&lt; 8 | digest[h * 4] &amp; 0xFF;
                        this.getSessionList(sessionMap, k).add(session);
                    }

                }
            } else {
                ...
            }
        }
        this.ketamaSessions = sessionMap;
        this.maxTries = list.size();
    }

    private List&lt;Session&gt; getSessionList(TreeMap&lt;Long, List&lt;Session&gt;&gt; sessionMap, long k) {
        List&lt;Session&gt; sessionList = sessionMap.get(k);
        if (sessionList == null) {
            sessionList = new ArrayList&lt;Session&gt;();
            sessionMap.put(k, sessionList);
        }
        return sessionList;
    }
</code></pre>

<p>buildMap的结果就是在0-2<sup>32</sup>这个圈上生成一些列的虚拟节点,每个虚拟节点都指向一个真实服务器.</p>

<p>例如本机只开一个服务器情况下,没有设置权重,会生成160个虚拟节点,每个虚拟节点都指向中一台服务器:</p>

<p><img src="/images/xmemcached/xmemcached-ketama-buildmap.png">
<strong>4.4.2 对Key做Hash</strong></p>

<p>做get或者set的时候,首先都会通过key找到服务器:</p>

<pre><code class="java">public Session getSessionByKey(final String key);
</code></pre>

<p>其实现代码如下:</p>

<pre><code class="java">//通过key找真实到服务器
public final Session getSessionByKey(final String key) {
    if (this.ketamaSessions == null || this.ketamaSessions.size() == 0) {
        return null;
    }
    long hash = this.hashAlg.hash(key);         //计算key的Hash值
    Session rv = this.getSessionByHash(hash);   //通过Hash值找服务器
    int tries = 0;
    while (!this.failureMode &amp;&amp; (rv == null || rv.isClosed())
            &amp;&amp; tries++ &lt; this.maxTries) {
        hash = this.nextHash(hash, key, tries);
        rv = this.getSessionByHash(hash);
    }
    return rv;
}

public final Session getSessionByHash(final long hash) {
    TreeMap&lt;Long, List&lt;Session&gt;&gt; sessionMap = this.ketamaSessions;
    if (sessionMap.size() == 0) {
        return null;
    }
    Long resultHash = hash;
    if (!sessionMap.containsKey(hash)) {
        //下面的逻辑是找到大于hash值的第一个虚拟节点(即ceiling)
        //Java 1.6使用ceilingKey可以实现,为兼容jdk5,使用tailMap,tailMap为所有大于hash值的虚拟节点
        SortedMap&lt;Long, List&lt;Session&gt;&gt; tailMap = sessionMap.tailMap(hash);
        if (tailMap.isEmpty()) {//没有比hash值大的节点,则取第一个虚拟节点
            resultHash = sessionMap.firstKey(); 
        } else {
            resultHash = tailMap.firstKey();//取tailMap第一个,即最大于hash值最小节点
        }
    }

    List&lt;Session&gt; sessionList = sessionMap.get(resultHash);
    if (sessionList == null || sessionList.size() == 0) {
        return null;
    }
    int size = sessionList.size();
    return sessionList.get(this.random.nextInt(size));
}
</code></pre>

<p>这里调试时候key是'a',其Hash值是3111502092:</p>

<p><img src="/images/xmemcached/xmemcached-ketama-key.png">
getSessionByHash的逻辑其实就是在Hash环上找第一个大于key对应hash值的虚拟节点,通过虚拟节点找到真实服务器.</p>

<p>这里的tailMap即为大于3111502092的虚拟节点,如下:</p>

<p><img src="/images/xmemcached/xmemcached-ketama-tailMap.png"></p>

<p>因此我们要找的虚拟节点就是3164521287对应的虚拟节点,其服务器指向的是127.0.0.1:11211.因此当前key(这里为'a')的请求被打到这台服务器上.</p>

<h3>4.5 使用XMemcached的一致性哈希</h3>

<p>默认情况下XMemcached使用的是余数哈希,如下使用一致性哈希:</p>

<pre><code>    MemcachedClientBuilder builder = new XMemcachedClientBuilder(AddrUtil.getAddresses(servers));
    // 设置使用一致性hash
    builder.setSessionLocator(new KetamaMemcachedSessionLocator());
    return builder.build();
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Memcached内存存储]]></title>
    <link href="http://xiaobaoqiu.github.io/blog/2014/10/15/memcachednei-cun-cun-chu/"/>
    <updated>2014-10-15T18:13:49+08:00</updated>
    <id>http://xiaobaoqiu.github.io/blog/2014/10/15/memcachednei-cun-cun-chu</id>
    <content type="html"><![CDATA[<p>早就听说过Memcached独特的内存管理方式,写着篇文章的目的就是了解Memcached的内存管理,学习其源代码.</p>

<h1>1.什么是Slab Allocator</h1>

<p>memcached默认情况下采用了名为Slab Allocator的机制分配、管理内存，Slab Allocator的基本原理是按照预先规定的大小，将分配的内存分割成特定长度的块，以期望完全解决内存碎片问题。而且，slab allocator还有重复使用已分配的内存的目的。 也就是说，分配到的内存不会释放，而是重复利用。</p>

<p><img src="/images/memcached/memcached-slab.png"></p>

<h1>2.Slab Allocation的主要术语</h1>

<pre><code>Page        分配给Slab的内存空间,默认是1MB,分配给Slab之后根据slab的大小切分成chunk
Chunk       用于缓存记录的内存空间
Slab Class  特定大小的chunk的组
</code></pre>

<p><img src="/images/memcached/memcached-slab-page.png"></p>

<h1>3.Slab初始化</h1>

<p>在Memcached启动时候会调用slab的初始化代码(详见memcached.c中main函数调用slabs_init函数).</p>

<p>slabs_init函数声明:</p>

<pre><code class="c">/** Init the subsystem. 1st argument is the limit on no. of bytes to allocate,
    0 if no limit. 2nd argument is the growth factor; each slab will use a chunk
    size equal to the previous slab's chunk size times this factor.
    3rd argument specifies if the slab allocator should allocate all memory
    up front (if true), or allocate memory in chunks as it is needed (if false)
*/
void slabs_init(const size_t limit, const double factor, const bool prealloc);
</code></pre>

<p>其中limit表示memcached最大使用内存;factor表示slab中chunk size的增长因子,slab中chunk size的大小等于前一个slab的chunk size乘以factor;</p>

<p>memcached.c中main函数调用slabs_init函数:</p>

<pre><code class="c">slabs_init(settings.maxbytes, settings.factor, preallocate);
</code></pre>

<p>其中settings.maxbytes默认值为64M,启动memcached使用选项-m设置;settings.factor默认为1.25,启动memcached时候使用-f设置;preallocate指的是启动memcached的时候默认为每种类型slab预先分配一个page的内存,默认是false;</p>

<pre><code>settings.maxbytes = 64 * 1024 * 1024; /* default is 64MB */
...
settings.factor = 1.25;
...
preallocate = false
</code></pre>

<p>slabs_init函数实现:</p>

<pre><code class="c">/**
 * Determines the chunk sizes and initializes the slab class descriptors
 * accordingly.
 */
void slabs_init(const size_t limit, const double factor, const bool prealloc) {
    int i = POWER_SMALLEST - 1;
    //真实占用大小=对象大小+48
    unsigned int size = sizeof(item) + settings.chunk_size;

    mem_limit = limit;

    //开启预分配,则首先将limit大小(默认64M)的内存全部申请
    if (prealloc) {
        /* Allocate everything in a big chunk with malloc */
        mem_base = malloc(mem_limit);
        if (mem_base != NULL) {
            mem_current = mem_base;
            mem_avail = mem_limit;
        } else {
            fprintf(stderr, "Warning: Failed to allocate requested memory in"
                    " one large chunk.\nWill allocate in smaller chunks\n");
        }
    }

    //清空所有的slab
    memset(slabclass, 0, sizeof(slabclass));

    while (++i &lt; POWER_LARGEST &amp;&amp; size &lt;= settings.item_size_max / factor) {
        /* Make sure items are always n-byte aligned */
        if (size % CHUNK_ALIGN_BYTES)
            size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES);

        slabclass[i].size = size;
        slabclass[i].perslab = settings.item_size_max / slabclass[i].size;
        size *= factor;
        if (settings.verbose &gt; 1) {
            fprintf(stderr, "slab class %3d: chunk size %9u perslab %7u\n",
                    i, slabclass[i].size, slabclass[i].perslab);
        }
    }

    //最大chunksize的一个slab,chunksize为settings.item_size_max(默认1M)
    power_largest = i;
    slabclass[power_largest].size = settings.item_size_max;
    slabclass[power_largest].perslab = 1;
    if (settings.verbose &gt; 1) {
        fprintf(stderr, "slab class %3d: chunk size %9u perslab %7u\n",
                i, slabclass[i].size, slabclass[i].perslab);
    }

    //记录已分配的空间大小
    /* for the test suite:  faking of how much we've already malloc'd */
    {
        char *t_initial_malloc = getenv("T_MEMD_INITIAL_MALLOC");
        if (t_initial_malloc) {
            mem_malloced = (size_t)atol(t_initial_malloc);
        }
    }

    //开启了预分配,则为每种slab都分配一个page的空间
    if (prealloc) {
        slabs_preallocate(power_largest);
    }
}
</code></pre>

<p>其中settings.chunk_size默认为48:</p>

<pre><code>settings.chunk_size = 48;         /* space for a modest key and value */
</code></pre>

<p>POWER_LARGEST指slab种类的最大值,默认只为200,在memcached.c中设置</p>

<pre><code>#define POWER_LARGEST  200
</code></pre>

<p>settings.item_size_max就是每个page的大小,默认1M,在memcached.c中初始化:</p>

<pre><code>settings.item_size_max = 1024 * 1024; /* The famous 1MB upper limit. */
</code></pre>

<p>默认不开启预分配,因为很多时候Memcached只存储一种类型的数据(即其大小相对比较固定),这时候其他类型的预分配的slab空间就会浪费.</p>

<p>预分配的逻辑就是从最小的slab开始,为每类slab分配一个Page大小的空间(空间不足时停止分配):</p>

<pre><code class="c">static void slabs_preallocate (const unsigned int maxslabs) {
    int i;
    unsigned int prealloc = 0;

    /* pre-allocate a 1MB slab in every size class so people don't get
       confused by non-intuitive "SERVER_ERROR out of memory"
       messages.  this is the most common question on the mailing
       list.  if you really don't want this, you can rebuild without
       these three lines.  */

    for (i = POWER_SMALLEST; i &lt;= POWER_LARGEST; i++) {
        if (++prealloc &gt; maxslabs)
            return;
        if (do_slabs_newslab(i) == 0) {
            fprintf(stderr, "Error while preallocating slab memory!\n"
                "If using -L or other prealloc options, max memory must be "
                "at least %d megabytes.\n", power_largest);
            exit(1);
        }
    }

}
</code></pre>

<p>do_slabs_newslab的工作就是为某一个slab分配空间,并将空间划分乘固定大小的chunk:</p>

<pre><code class="c">static int do_slabs_newslab(const unsigned int id) {
    slabclass_t *p = &amp;slabclass[id];
    int len = settings.slab_reassign ? settings.item_size_max
        : p-&gt;size * p-&gt;perslab;
    char *ptr;

    if ((mem_limit &amp;&amp; mem_malloced + len &gt; mem_limit &amp;&amp; p-&gt;slabs &gt; 0) ||
        (grow_slab_list(id) == 0) ||
        ((ptr = memory_allocate((size_t)len)) == 0)) {  //申请内存

        MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id);
        return 0;
    }

    memset(ptr, 0, (size_t)len);
    //将内存划分乘chunk
    split_slab_page_into_freelist(ptr, id);

    //维护slab链表
    p-&gt;slab_list[p-&gt;slabs++] = ptr;
    mem_malloced += len;
    MEMCACHED_SLABS_SLABCLASS_ALLOCATE(id);

    return 1;
}
</code></pre>

<p>split_slab_page_into_freelist的主要控制就是Page划分乘chunk并清空:</p>

<pre><code class="c">static void split_slab_page_into_freelist(char *ptr, const unsigned int id) {
    slabclass_t *p = &amp;slabclass[id];
    int x;
    for (x = 0; x &lt; p-&gt;perslab; x++) {
        do_slabs_free(ptr, 0, id);
        ptr += p-&gt;size;
    }
}
</code></pre>

<p>memcached的内存分配策略就是：按slab需求分配page,各slab按需使用chunk存储.</p>

<p>按需分配的意思就是某一类slab没有对象可存,就不会分配(非preallocate模式),某类slab存储对象很多,就会分配多个slab形成链表.</p>

<p>这里有几个特点要注意:</p>

<pre><code>1.Memcached分配出去的page不会被回收或者重新分配;
2.Memcached申请的内存不会被释放;
3.slab空闲的chunk不会借给任何其他slab使用(新版本memcached有slab_reassign,slab_automove的功能);
</code></pre>

<p>slab内存结构图,二维数组链表:</p>

<p><img src="/images/memcached/memcached_slab_alloct.jpg"></p>

<h1>4.往Slab中缓存记录</h1>

<p>memcached根据收到的数据的大小,选择最适合数据大小的slab.
memcached中保存着slab内空闲chunk的列表,根据该列表选择chunk,
然后将数据缓存于其中.</p>

<p><img src="/images/memcached/memcached-slab-save.png"></p>

<p>代码如下:</p>

<pre><code class="c">/*
 * Figures out which slab class (chunk size) is required to store an item of
 * a given size.
 *
 * Given object size, return id to use when allocating/freeing memory for object
 * 0 means error: can't store such a large object
 */
unsigned int slabs_clsid(const size_t size) {
    int res = POWER_SMALLEST;   //最小slab编号

    if (size == 0)
        return 0;
    while (size &gt; slabclass[res].size)
        if (res++ == power_largest)     /* won't fit in the biggest slab */
            return 0;
    return res;
}
</code></pre>

<p>参数是待存储对象的大小,根据这个大小,从最小的Chunk Size开始查找,找到第一个(即最小的)能放下size大小的对象的Chunk.找不到(size大于最大的Chunk Size)返回0(这就是为什么slab class从1开始而不是从0开始).</p>

<p>如果某个Slab没有剩余的Chunk了，系统便会给这个Slab分配一个新的Page以供使用，如果没有Page可用，系统就会触发LRU机制，通过删除冷数据来为新数据腾出空间，这里有一点需要注意的是：LRU不是全局的，而是针对Slab而言的.</p>

<p>slab内存分配示例:</p>

<p><img src="/images/memcached/memcached_slab_ins.jpg"></p>

<h1>5.Slab Allocator的缺点</h1>

<p>由于Slab Allocator分配的是特定长度的内存，因此无法有效利用分配的内存。 例如，将100字节的数据缓存到128字节的chunk中，剩余的28字节就浪费了。</p>

<p><img src="/images/memcached/memcached-slab-disadvantage.png"></p>

<h1>6.Memcached减少内存浪费</h1>

<h3>4.1:调整growth factor</h3>

<pre><code>(1).估算我们item的大小
key键长＋suffix+value值长＋结构大小(48字节)
(2).逐步调整growth factor,使得某个slab的大小和我们的item大小接近(必须大于我们item的大小)
</code></pre>

<h1>7.过期数据</h1>

<pre><code>(1).LRU过期策略;
(2).在slab级别上执行LRU策略;
(3).查看是否过去是在get的时候,即懒惰(lazy)检查;
</code></pre>

<h1>8.memcached-tool脚本</h1>

<p>memcached-tool脚本可以方便地获得slab的使用情况 （它将memcached的返回值整理成容易阅读的格式）,可以从下面的地址获得脚本:
<a href="http://www.netingcn.com/demo/memcached-tool.zip">http://www.netingcn.com/demo/memcached-tool.zip</a></p>

<p>使用方法也极其简单：</p>

<pre><code>perl memcached-tool server_ip:prot option
</code></pre>

<p>比如:</p>

<pre><code>perl memcached-tool 10.0.0.5:11211 display    # shows slabs
perl memcached-tool 10.0.0.5:11211            # same.  (default is display)
perl memcached-tool 10.0.0.5:11211 stats      # shows general stats
perl memcached-tool 10.0.0.5:11211 move 7 9   # takes 1MB slab from class #7
                                              # to class #9.
</code></pre>

<p>输出示例:</p>

<pre><code>#  Item_Size   Max_age  1MB_pages Count   Full?
 1     104 B  1394292 s    1215 12249628    yes
 2     136 B  1456795 s      52  400919     yes
 ...
</code></pre>

<p>各列的含义为：</p>

<pre><code>#           slab class编号
Item_Size   Chunk大小
Max_age     LRU内最旧的记录的生存时间
1MB_pages   分配给Slab的页数
Count       Slab内的记录数
Full?       Slab内是否含有空闲chunk
</code></pre>
]]></content>
  </entry>
  
</feed>
