<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Memcached | xiaobaoqiu Blog]]></title>
  <link href="http://xiaobaoqiu.github.io/blog/categories/memcached/atom.xml" rel="self"/>
  <link href="http://xiaobaoqiu.github.io/"/>
  <updated>2015-08-01T15:16:02+08:00</updated>
  <id>http://xiaobaoqiu.github.io/</id>
  <author>
    <name><![CDATA[xiaobaoqiu]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Memcached网络模型]]></title>
    <link href="http://xiaobaoqiu.github.io/blog/2014/11/03/memcachedwang-luo-mo-xing/"/>
    <updated>2014-11-03T18:59:33+08:00</updated>
    <id>http://xiaobaoqiu.github.io/blog/2014/11/03/memcachedwang-luo-mo-xing</id>
    <content type="html"><![CDATA[<p>这篇文章的目的是学习Memcached网络模型相关的源代码.</p>

<p>Memcached采用了很典型的Master-Worker模型，采用的是多线程而不是多进程. 主线程(Master)接收连接, 然后把连接平分派给工作线程(Worker),工作线程处理业务逻辑.</p>

<p>核心的共享数据是消息队列，主线程会把收到的事件请求放入队列，随后调度程序会选择一个空闲的Worker线程来从队列中取出事件请求进行处理.</p>

<h1>1.libevent简介</h1>

<p>Memcached使用libevent实现事件循环，libevent在Linux环境下默认采用epoll作为IO多路复用方法.
用户线程使用libevent则通常按以下步骤：
    (1).用户线程通过event_init()函数创建一个event_base对象。event_base对象管理所有注册到自己内部的IO事件。多线程环境下，event_base对象不能被多个线程共享，即一个event_base对象只能对应一个线程。
    (2).然后该线程通过event_add函数，将与自己感兴趣的文件描述符相关的IO事件，注册到event_base对象，同时指定事件发生时所要调用的事件处理函数（event handler）。服务器程序通常监听套接字（socket）的可读事件。比如，服务器线程注册套接字sock1的EV_READ事件，并指定event_handler1()为该事件的回调函数。libevent将IO事件封装成struct event类型对象，事件类型用EV_READ/EV_WRITE等常量标志。
    (3).注册完事件之后，线程调用event_base_loop进入循环监听（monitor）状态。该循环内部会调用epoll等IO复用函数进入阻塞状态，直到描述符上发生自己感兴趣的事件。此时，线程会调用事先指定的回调函数处理该事件。例如，当套接字sock1发生可读事件，即sock1的内核buff中已有可读数据时，被阻塞的线程立即返回（wake up）并调用event_handler1()函数来处理该次事件。
    (4).处理完这次监听获得的事件后，线程再次进入阻塞状态并监听，直到下次事件发生。</p>

<h1>2.Memcached网络模型</h1>

<p>大致的图示如下:</p>

<p><img src="/images/memcached/memcached-libevent.jpg"></p>

<h3>2.1主要数据结构</h3>

<p>首先是CQ_ITEM, CQ_ITEM实际上是主线程accept后返回的已建立连接的fd的封装:</p>

<pre><code class="c thread.c">/* An item in the connection queue. */
typedef struct conn_queue_item CQ_ITEM;
struct conn_queue_item {
    int               sfd;
    enum conn_states  init_state;
    int               event_flags;
    int               read_buffer_size;
    enum network_transport     transport;
    CQ_ITEM          *next;
};
</code></pre>

<p>CQ是一个管理CQ_ITEM的单向链表:</p>

<pre><code class="c thread.c">/* A connection queue. */
typedef struct conn_queue CQ;
struct conn_queue {
    CQ_ITEM *head;
    CQ_ITEM *tail;
    pthread_mutex_t lock;
};
</code></pre>

<p>LIBEVENT_THREAD是Memcached对线程结构的封装,每个线程都包含一个CQ队列，一条通知管道pipe
和一个libevent的实例event_base :</p>

<pre><code>typedef struct {
    pthread_t thread_id;        /* unique ID of this thread */
    struct event_base *base;    /* libevent handle this thread uses */
    struct event notify_event;  /* listen event for notify pipe */
    int notify_receive_fd;      /* receiving end of notify pipe */
    int notify_send_fd;         /* sending end of notify pipe */
    struct thread_stats stats;  /* Stats generated by this thread */
    struct conn_queue *new_conn_queue; /* CQ队列 */
    cache_t *suffix_cache;      /* suffix cache */
    uint8_t item_lock_type;     /* use fine-grained or global item lock */
} LIBEVENT_THREAD;
</code></pre>

<h3>2.2主流程</h3>

<p>在memcached.c的main函数中展示了客户端请求处理的主流程:</p>

<p><strong>(1).对主线程的libevent做了初始化</strong></p>

<pre><code class="c">/* initialize main thread libevent instance */  
 main_base = event_init();
</code></pre>

<p><strong>(2).初始化所有的线程(包括Master和Worker线程),并启动</strong></p>

<pre><code class="c">/* start up worker threads if MT mode */  
thread_init(settings.num_threads, main_base);
</code></pre>

<p>其中settings.num_threads表示线程数目,默认是4个:</p>

<pre><code class="c">settings.num_threads = 4;         /* N workers */
</code></pre>

<p>下面简单分析thread_init的核心代码(thread.c):</p>

<pre><code class="c">/*
 * Initializes the thread subsystem, creating various worker threads.
 *
 * nthreads  Number of worker event handler threads to spawn
 * main_base Event base for main thread
 */
void thread_init(int nthreads, struct event_base *main_base) {

    ...//省略若干代码

    //threads的声明在thread.c头部,用于保存所有的线程
    threads = calloc(nthreads, sizeof(LIBEVENT_THREAD));
    if (! threads) {
        perror("Can't allocate thread descriptors");
        exit(1);
    }

    dispatcher_thread.base = main_base;
    dispatcher_thread.thread_id = pthread_self();

    for (i = 0; i &lt; nthreads; i++) {
        int fds[2];
        if (pipe(fds)) {    //创建管道
            perror("Can't create notify pipe");
            exit(1);
        }

        threads[i].notify_receive_fd = fds[0];  //读端
        threads[i].notify_send_fd = fds[1];     //写端

        //创建所有workers线程的libevent实例
        setup_thread(&amp;threads[i]);
        /* Reserve three fds for the libevent base, and two for the pipe */
        stats.reserved_fds += 5;
    }

    //创建线程
    /* Create threads after we've done all the libevent setup. */
    for (i = 0; i &lt; nthreads; i++) {
        create_worker(worker_libevent, &amp;threads[i]);
    }

    //等待所有线程启动起来之后,这个函数再返回
    /* Wait for all the threads to set themselves up before returning. */
    pthread_mutex_lock(&amp;init_lock);
    wait_for_thread_registration(nthreads);
    pthread_mutex_unlock(&amp;init_lock);
}
</code></pre>

<p>thread_init首先malloc线程的空间，然后第一个threads作为主线程，其余都是workers线程
然后为每个线程创建一个pipe，这个pipe被用来作为主线程通知workers线程有新的连接到达.</p>

<p>其中pipe()函数用于创建管道,管道两端可分别用描述字fds[0]以及fds[1]来描述.需要注意的是，管道的两端是固定的。即一端只能用于读，由描述字fds[0]表示，称其为管道读端；另一端则只能用于写，由描述字fds[1]来表示，称其为管道写端.</p>

<p>setup_thread主要是创建所有workers线程的libevent实例（主线程的libevent实例在main函数中已经建立）,setup_thread()的代码如下:</p>

<pre><code class="c">/*
 * Set up a thread's information.
 */
static void setup_thread(LIBEVENT_THREAD *me) {
    me-&gt;base = event_init();
    if (! me-&gt;base) {
        fprintf(stderr, "Can't allocate event base\n");
        exit(1);
    }

    //注意这里只有notify_receive_fd,即读端口
    /* Listen for notifications from other threads */
    event_set(&amp;me-&gt;notify_event, me-&gt;notify_receive_fd,
              EV_READ | EV_PERSIST, thread_libevent_process, me);
    event_base_set(me-&gt;base, &amp;me-&gt;notify_event);

    if (event_add(&amp;me-&gt;notify_event, 0) == -1) {
        fprintf(stderr, "Can't monitor libevent notify pipe\n");
        exit(1);
    }

    me-&gt;new_conn_queue = malloc(sizeof(struct conn_queue));
    if (me-&gt;new_conn_queue == NULL) {
        perror("Failed to allocate memory for connection queue");
        exit(EXIT_FAILURE);
    }
    cq_init(me-&gt;new_conn_queue);

    ...
}
</code></pre>

<p>这里会为所有worker thread线程注册与notify_event_fd描述符有关的IO事件，这里的notify_event_fd描述符是该worker thread线程与main thread线程通信的管道的接收端(读)描述符。通过注册与该描述符有关的IO事件，worker thread线程就能监听main thread线程发给自己的数据(即事件).</p>

<p>注意这里event_set中的thread_libevent_process参数,其意义在于监听Worker线程与main thread线程通信的管道上的可读事件，并指定用thread_libevent_process()函数处理该事件,即每次管道读端有数据刻度,即触发thread_libevent_process过程.</p>

<p>thread_libevent_process的代码如下,其中最重要的一个就是数据为c的,后续会详细分析这块代码.</p>

<pre><code class="c">/*
 * Processes an incoming "handle a new connection" item. This is called when
 * input arrives on the libevent wakeup pipe.
 */
static void thread_libevent_process(int fd, short which, void *arg) {
    LIBEVENT_THREAD *me = arg;
    CQ_ITEM *item;
    char buf[1];

    //从管道中读数据
    if (read(fd, buf, 1) != 1)
        if (settings.verbose &gt; 0)
            fprintf(stderr, "Can't read from libevent pipe\n");

    switch (buf[0]) {
    case 'c':   //c表示有新的连接请求被主线程分配到当前Worker线程
    item = cq_pop(me-&gt;new_conn_queue);

    if (NULL != item) {
        conn *c = conn_new(item-&gt;sfd, item-&gt;init_state, item-&gt;event_flags,
                           item-&gt;read_buffer_size, item-&gt;transport, me-&gt;base);
        if (c == NULL) {
            if (IS_UDP(item-&gt;transport)) {
                fprintf(stderr, "Can't listen for events on UDP socket\n");
                exit(1);
            } else {
                if (settings.verbose &gt; 0) {
                    fprintf(stderr, "Can't listen for events on fd %d\n",
                        item-&gt;sfd);
                }
                close(item-&gt;sfd);
            }
        } else {
            c-&gt;thread = me;
        }
        cqi_free(item);
    }
        break;
    /* we were told to flip the lock type and report in */
    case 'l':
        ...
    case 'g':
        ...
    }
}
</code></pre>

<p>thread_init函数中create_worker实际上就是真正启动了线程, create_worker的代码如下:</p>

<pre><code class="c">/*
 * Creates a worker thread.
 */
static void create_worker(void *(*func)(void *), void *arg) {
    pthread_t       thread;
    pthread_attr_t  attr;
    int             ret;

    pthread_attr_init(&amp;attr);

    if ((ret = pthread_create(&amp;thread, &amp;attr, func, arg)) != 0) {
        fprintf(stderr, "Can't create thread: %s\n",
                strerror(ret));
        exit(1);
    }
}
</code></pre>

<p>pthread_create是创建线程函数,第三个参数是线程运行函数的起始地址,这里即worker_libevent函数,该方法执行event_base_loop启动该线程的libevent.</p>

<pre><code class="c">/*
 * Worker thread: main event loop
 */
static void *worker_libevent(void *arg) {
    LIBEVENT_THREAD *me = arg;

    /* Any per-thread setup can happen here; thread_init() will block until
     * all threads have finished initializing.
     */

    /* set an indexable thread-specific memory item for the lock type.
     * this could be unnecessary if we pass the conn *c struct through
     * all item_lock calls...
     */
    me-&gt;item_lock_type = ITEM_LOCK_GRANULAR;
    pthread_setspecific(item_lock_type_key, &amp;me-&gt;item_lock_type);

    register_thread_initialized();

    event_base_loop(me-&gt;base, 0);
    return NULL;
}
</code></pre>

<p>这里我们需要记住每个workers线程目前只在自己线程的管道的读端有数据时可读时触发，并调用
thread_libevent_process方法.</p>

<p><strong>(3).主线程调用</strong></p>

<pre><code class="c">/* create the listening socket, bind it, and init */
server_sockets(settings.port, tcp_transport, portnumber_file) 
</code></pre>

<p>在worker thread线程启动后，main thread线程就要创建监听套接字（listening socket）来等待客户端连接请求。这个方法主要是封装了创建监听socket，绑定地址，设置非阻塞模式并注册监听socket的libevent 读事件等一系列操作.</p>

<p>套接字被封装成conn对象，表示与客户端的连接,定义十分庞大(见memcached.h).</p>

<p>端口号默认是11211:</p>

<pre><code class="c">settings.port = 11211;
</code></pre>

<p>server_sockets函数主要调用server_socket()函数:</p>

<pre><code class="c">/**
 * Create a socket and bind it to a specific port number
 * @param interface the interface to bind to
 * @param port the port number to bind to
 * @param transport the transport protocol (TCP / UDP)
 * @param portnumber_file A filepointer to write the port numbers to
 *        when they are successfully added to the list of ports we
 *        listen on.
 */
static int server_socket(const char *interface,
                         int port,
                         enum network_transport transport,
                         FILE *portnumber_file) {

    ...//省略若干代码

    //主机名到地址解析,结果存在ai中,为addrinfo的链表
    error= getaddrinfo(interface, port_buf, &amp;hints, &amp;ai);
    if (error != 0) {
        if (error != EAI_SYSTEM)
          fprintf(stderr, "getaddrinfo(): %s\n", gai_strerror(error));
        else
          perror("getaddrinfo()");
        return 1;
    }

    for (next= ai; next; next= next-&gt;ai_next) {
        conn *listen_conn_add;
        if ((sfd = new_socket(next)) == -1) {   //创建socket
            /* getaddrinfo can return "junk" addresses,
             * we make sure at least one works before erroring.
             */
            if (errno == EMFILE) {
                /* ...unless we're out of fds */
                perror("server_socket");
                exit(EX_OSERR);
            }
            continue;
        }
        //IPV4地址,设置socket选项
        setsockopt(sfd, SOL_SOCKET, SO_REUSEADDR, (void *)&amp;flags, sizeof(flags));
        ...//省略若干代码

        //socket和地址绑定
        if (bind(sfd, next-&gt;ai_addr, next-&gt;ai_addrlen) == -1) {
            if (errno != EADDRINUSE) {
                perror("bind()");
                close(sfd);
                freeaddrinfo(ai);
                return 1;
            }
            close(sfd);
            continue;
        } else {
            success++;
            ...//省略若干代码
        }

        if (IS_UDP(transport)) {
            ...//省略若干代码
        } else {
            if (!(listen_conn_add = conn_new(sfd, conn_listening,
                                             EV_READ | EV_PERSIST, 1,
                                             transport, main_base))) {
                fprintf(stderr, "failed to create listening connection\n");
                exit(EXIT_FAILURE);
            }
            listen_conn_add-&gt;next = listen_conn;
            listen_conn = listen_conn_add;
        }
    }

    freeaddrinfo(ai);

    /* Return zero iff we detected no errors in starting up connections */
    return success == 0;
}
</code></pre>

<p>conn_new()是这里的最关键的一个函数,此函数负责将原始套接字封装成为一个conn对象，同时会注册与该conn对象相关的IO事件，并指定该连接（conn）的初始状态。这里要注意的是listening socket的conn对象被初始化为conn_listening状态.</p>

<pre><code class="c">conn *conn_new(const int sfd, enum conn_states init_state,
                const int event_flags,
                const int read_buffer_size, enum network_transport transport,
                struct event_base *base) {

    ...//省略若干代码

    //设置fd和初始状态
    c-&gt;sfd = sfd;                
    c-&gt;state = init_state;

    //注册与该连接有关的IO事件
    event_set(&amp;c-&gt;event, sfd, event_flags, event_handler, (void *)c);
    event_base_set(base, &amp;c-&gt;event);
    c-&gt;ev_flags = event_flags;

    if (event_add(&amp;c-&gt;event, 0) == -1) {
        perror("event_add");
        return NULL;
    }

    ...//
}
</code></pre>

<p>所有conn对象IO事件相关的处理函数都是event_handler()函数,这个函数主要是调用drive_machine()函数:</p>

<pre><code class="c">void event_handler(const int fd, const short which, void *arg) {
    conn *c;
    ...
    drive_machine(c);
    ...
}
</code></pre>

<p>drive_machine这个函数就全权负责处理与客户连接相关的事件:</p>

<pre><code class="c">static void drive_machine(conn *c) {
    ...
    assert(c != NULL);

    while (!stop) {
        switch(c-&gt;state) {
        case conn_listening:
            addrlen = sizeof(addr);
            sfd = accept(c-&gt;sfd, (struct sockaddr *)&amp;addr, &amp;addrlen);
            ...

            if (settings.maxconns_fast &amp;&amp;
                ...
            } else {
                dispatch_conn_new(sfd, conn_new_cmd, EV_READ | EV_PERSIST,
                                     DATA_BUFFER_SIZE, tcp_transport);
            }

            stop = true;
            break;
        case conn_waiting:
        ...
        case conn_read:
        ...
        case conn_parse_cmd:
        ...
        case conn_new_cmd:
        ...
        case conn_nread:
        ...
        case conn_swallow:
        ...
        case conn_write:
        ...
        case conn_mwrite:
        ...
        case conn_closing:
        ...
        case conn_closed:
    ...
}
</code></pre>

<p>drive_machine中就是conn对象的state字段发挥作用的地方了,drive_machine()函数是一个巨大的switch语句，它根据conn对象的当前状态，即state字段的值选择执行不同的分支，因为listening socket的conn对象被初始化为conn_listening状态，所以drive_machine()函数会执行switch语句中case conn_listenning的分支，即接受客户端连接并通过dispatch_conn_new()函数将连接分派给Worker线程.</p>

<p>dispatch_conn_new代码如下(thread.c):</p>

<pre><code class="c">/*
 * Dispatches a new connection to another thread. This is only ever called
 * from the main thread, either during initialization (for UDP) or because
 * of an incoming connection.
 */
void dispatch_conn_new(int sfd, enum conn_states init_state, int event_flags,
                       int read_buffer_size, enum network_transport transport) {
    CQ_ITEM *item = cqi_new();  //新申请一个CQ_ITEM
    char buf[1];
    if (item == NULL) {
        close(sfd);
        /* given that malloc failed this may also fail, but let's try */
        fprintf(stderr, "Failed to allocate memory for connection object\n");
        return ;
    }

    //分发给Worker线程
    int tid = (last_thread + 1) % settings.num_threads;

    LIBEVENT_THREAD *thread = threads + tid;

    last_thread = tid;

    item-&gt;sfd = sfd;
    item-&gt;init_state = init_state;  //注意这里的状态为conn_new_cmd
    item-&gt;event_flags = event_flags;
    item-&gt;read_buffer_size = read_buffer_size;
    item-&gt;transport = transport;

    //把新申请的CQ_ITEM放到被分配的Worker线程的队列中
    cq_push(thread-&gt;new_conn_queue, item);

    MEMCACHED_CONN_DISPATCH(sfd, thread-&gt;thread_id);
    buf[0] = 'c';
    //向worker thread线程的管道写入一字节的数据
    if (write(thread-&gt;notify_send_fd, buf, 1) != 1) {
        perror("Writing to thread notify pipe");
    }
}
</code></pre>

<p>向Worker线程写一个字符的意义在于触发Worker线程管道的读端，即notify_receive_fd描述符的可读事件.</p>

<p>主线程在新连接到来的时候是如何选择处理副线程的呢?很简单,有一个计数器last_thread, 每次将last_thread加一,再模线程数来选择线程ID.</p>

<p>通过之前的分析,我们知道,Worker线程的管道有读时间触发的时候,会调用thread_libevent_process来处理,这里详细分析一下:</p>

<pre><code class="c">/*
 * Processes an incoming "handle a new connection" item. This is called when
 * input arrives on the libevent wakeup pipe.
 */
static void thread_libevent_process(int fd, short which, void *arg) {
    LIBEVENT_THREAD *me = arg;
    CQ_ITEM *item;
    char buf[1];

    //从管道中读数据
    if (read(fd, buf, 1) != 1)
        if (settings.verbose &gt; 0)
            fprintf(stderr, "Can't read from libevent pipe\n");

    switch (buf[0]) {
    case 'c':   //c表示有新的连接请求被主线程分配到当前Worker线程
    //从当前Worker线程的连接请求队列中弹出一个请求
    //此对象即先前main thread线程推入new_conn_queue队列的对象
    item = cq_pop(me-&gt;new_conn_queue);

    if (NULL != item) {
        //根据这个CQ_ITEM对象，创建并初始化conn对象
        //该对象负责客户端与该worker thread线程之间的通信
        conn *c = conn_new(item-&gt;sfd, item-&gt;init_state, item-&gt;event_flags,
                           item-&gt;read_buffer_size, item-&gt;transport, me-&gt;base);
        if (c == NULL) {
            if (IS_UDP(item-&gt;transport)) {
                fprintf(stderr, "Can't listen for events on UDP socket\n");
                exit(1);
            } else {
                if (settings.verbose &gt; 0) {
                    fprintf(stderr, "Can't listen for events on fd %d\n",
                        item-&gt;sfd);
                }
                close(item-&gt;sfd);
            }
        } else {
            c-&gt;thread = me;
        }
        cqi_free(item);
    }
        break;
    /* we were told to flip the lock type and report in */
    case 'l':
        ...
    case 'g':
        ...
    }
}
</code></pre>

<p>到这里,Worker线程就建立了和客户端的连接.</p>

<p>conn_new的一个值得注意的地方就是会设置线程的事件处理函数:</p>

<pre><code class="c">conn *conn_new(const int sfd, enum conn_states init_state,
                const int event_flags,
                const int read_buffer_size, enum network_transport transport,
                struct event_base *base) {
    ...
    event_set(&amp;c-&gt;event, sfd, event_flags, event_handler, (void *)c);
        event_base_set(base, &amp;c-&gt;event);
        c-&gt;ev_flags = event_flags;

        if (event_add(&amp;c-&gt;event, 0) == -1) {
            perror("event_add");
            return NULL;
        }
    ...
}
</code></pre>

<p>我们可以看到,Worker线程也是使用event_handler函数来处理客户端请求过来的数据,根当前请求连接的状态来处理.</p>

<p><strong>(4).事件循环</strong></p>

<pre><code class="c">/* enter the event loop */  
event_base_loop(main_base, 0);
</code></pre>

<p>这时主线程启动开始通过libevent来接受外部连接请求，整个启动过程完毕.</p>

<h1>3.总结</h1>

<p>Memcached中采用的就是所谓的半同步-半异步模式,最早应该是由ACE的作者提出,原文在<a href="http://www.cs.wustl.edu/~schmidt/PDF/HS-HA.pdf">这里</a>.</p>

<p>简单示意图如下:</p>

<p><img src="/images/memcached/half-sync-half-async.jpg"></p>

<h3>3.1半同步-半异步模式</h3>

<p>几个模块的之间的交互为:</p>

<pre><code>(1).异步模块接收可能会异步到来的各种事件(I/O,信号等),然后将它们放入队列中;
(2).同步模块一般只有一种动作,就是不停的从队列中取出消息进行处理;
</code></pre>

<p>半同步-半异步模式的出现是为了给服务器的功能进行划分,尽可能将的可能阻塞的操作放在同步模块中,这样不会影响到异步模块的处理.</p>

<p>举个例子说明:</p>

<p>假设现在有一个服务器,在接收完客户端请求之后会去数据库查询,这个查询可能会很慢.这时,如果还是采用的把接收客户端的连接和处理客户端的请求(在这里这个处理就是查询数据库)放在一个模块中来处理,很可能将会有很多连接的处理响应非常慢.</p>

<p>此时,考虑使用半同步半异步的模式,开一个进程,使用多路复用IO(如epoll/select)等监听客户端的连接,接收到新的连接请求之后就将这些请求存放到通过某种IPC方式实现的消息队列中,同时,还有N个处理进程,它们所做的工作就是不停的从消息队列中取出消息进行处理.这样的划分,将接收客户端请求和处理客户端请求划分为不同的模块,相互之间的通过IPC进行通讯,将对彼此功能的影响限制到最小.</p>

<p><strong>优点</strong></p>

<pre><code>(1).接收操作只在主循环中处理,因此不会出现惊群现象;
(2).主副线程分工明确, 主线程仅负责I/O, 副线程负责业务逻辑处理;
(3).多个副线程之间不会有影响,因为大家都有各自独立的连接队列;
</code></pre>

<p><strong>缺点</strong></p>

<p>假如业务逻辑是类似于web服务器之类的, 那么一个简单的请求也需要这个比较繁琐的操作的话(最重要的是,很可能一个进程就能处理完的事情,非得从一个线程接收再到另一个线程去处理), 那么显然代价是不值得的.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[XMemcached Client]]></title>
    <link href="http://xiaobaoqiu.github.io/blog/2014/10/16/xmemcached-client/"/>
    <updated>2014-10-16T22:51:01+08:00</updated>
    <id>http://xiaobaoqiu.github.io/blog/2014/10/16/xmemcached-client</id>
    <content type="html"><![CDATA[<h1>1.XMemcached是什么</h1>

<p>XMemcached是众多Memcached Client中的后起之秀,而且还是一个Chinese主导的.写这篇Blog的目的就是研究下这个Memcached Client的代码,顺便研究下传说中的一致性Hash.</p>

<p>XMemcached项目:<a href="https://code.google.com/p/xmemcached/">https://code.google.com/p/xmemcached/</a></p>

<p>源代码:<a href="https://github.com/killme2008/xmemcached">https://github.com/killme2008/xmemcached</a></p>

<p>使用文档:<a href="http://blog.sina.com.cn/s/blog_6094008a0102v6gj.html">http://blog.sina.com.cn/s/blog_6094008a0102v6gj.html</a></p>

<p>特点:</p>

<pre><code>(1).支持所有的Memcached文本协议(text based protocols)和二进制协议(二进制协议从1.2.0开始支持);
(2).支持分布式的Memcached,包括标准Hash和一致性哈希策略;
(3).支持JMX,从而允许使用者监控和控制XMemcached Client的行为;同时可以修改优化参数,动态添加或者删除服务器;
(4).支持待权重的服务器配置;
(5).支持连接池,使用Java的nio,对同一个Memcached服务器使用者能够创建更多的连接;
(6).支持故障模式和备用节点;
(7).支持和Spring框架和hibernate-memcached的整合;
(8).高性能;
(9).支持和kestrel(一个Scala实现的MQ)和TokyoTyrant的对话;
</code></pre>

<h1>2.XMemcached的类图</h1>

<p>XMemcached的类图(几个主要的类):</p>

<p><img src="/images/xmemcached/xmemcached-classes.jpg"></p>

<h1>3.XMemcached的示例</h1>

<p>源代码中自带了例子,我们分析一个SimpleExample.java:</p>

<pre><code class="java">public class SimpleExample {
    public static void main(String[] args) {
        if (args.length &lt; 1) {
            System.err.println("Useage:java SimpleExample [servers]");
            System.exit(1);
        }
        MemcachedClient memcachedClient = getMemcachedClient(args[0]);
        if (memcachedClient == null) {
            throw new NullPointerException("Null MemcachedClient,please check memcached has been started");
        }
        try {
            // add a,b,c
            System.out.println("Add a,b,c");
            memcachedClient.set("a", 0, "Hello,xmemcached");
            memcachedClient.set("b", 0, "Hello,xmemcached");
            memcachedClient.set("c", 0, "Hello,xmemcached");
            // get a
            String value = memcachedClient.get("a");
            System.out.println("get a=" + value);
            System.out.println("delete a");
            // delete a
            memcachedClient.delete("a");
            // reget a
            value = memcachedClient.get("a");
            System.out.println("after delete,a=" + value);

            System.out.println("Iterate all keys...");
            // iterate all keys
            KeyIterator it = memcachedClient.getKeyIterator(AddrUtil.getOneAddress(args[0]));
            while (it.hasNext()) {
                System.out.println(it.next());
            }
            System.out.println(memcachedClient.touch("b", 1000));

        } catch (MemcachedException e) {
            System.err.println("MemcachedClient operation fail");
            e.printStackTrace();
        } catch (TimeoutException e) {
            System.err.println("MemcachedClient operation timeout");
            e.printStackTrace();
        } catch (InterruptedException e) {
            // ignore
        }
        try {
            memcachedClient.shutdown();
        } catch (Exception e) {
            System.err.println("Shutdown MemcachedClient fail");
            e.printStackTrace();
        }
    }

    public static MemcachedClient getMemcachedClient(String servers) {
        try {
            //默认是Text协议
            MemcachedClientBuilder builder = new XMemcachedClientBuilder(AddrUtil.getAddresses(servers));
            return builder.build();
        } catch (IOException e) {
            System.err.println("Create MemcachedClient fail");
            e.printStackTrace();
        }
        return null;
    }
}
</code></pre>

<p>本地先起来两个Memcached实例:</p>

<pre><code>/usr/bin/memcached -m 256 -p 11211 -u memcache -l 127.0.0.1 -d
/usr/bin/memcached -m 256 -p 11222 -u memcache -l 127.0.0.1 -d
</code></pre>

<p>在IDEA下运行,配上运行参数,即Memcached服务器的IP和端口,格式如下(解析代码见AddrUtil.java):</p>

<pre><code>127.0.0.1:11211 127.0.0.1:11222
</code></pre>

<p>Debug代码,可以发现其运行过程,下面是get的一个运行序列图:</p>

<p><img src="/images/xmemcached/xmemcached-get.jpg"></p>

<h1>4.XMemcached的分布式实现</h1>

<p>memcached虽然称为“分布式”缓存服务器,但服务器端并没有"&ldquo;分布式”功能.至于memcached的分布式,则是完全由客户端程序库实现的.</p>

<h3>4.1 Memcached的分布式是什么意思</h3>

<p>这里多次使用了“分布式”这个词,但并未做详细解释.现在开始简单地介绍一下其原理,各个客户端的实现基本相同.</p>

<p>下面假设memcached服务器有node1～node3三台, 应用程序要保存键名为“tokyo”“kanagawa”“chiba”“saitama”“gunma” 的数据.</p>

<p><img src="/images/memcached/memcached-distribute-1.png"></p>

<p>首先向memcached中添加“tokyo”.将“tokyo”传给客户端程序库后, 客户端实现的算法就会根据“键”来决定保存数据的memcached服务器. 服务器选定后,即命令它保存“tokyo”及其值.</p>

<p><img src="/images/memcached/memcached-distribute-2.png"></p>

<p>同样,“kanagawa”“chiba”“saitama”“gunma”都是先选择服务器再保存.</p>

<p>接下来获取保存的数据.获取时也要将要获取的键“tokyo”传递给函数库. 函数库通过与数据保存时相同的算法,根据“键”选择服务器. 使用的算法相同,就能选中与保存时相同的服务器,然后发送get命令. 只要数据没有因为某些原因被删除,就能获得保存的值.</p>

<p><img src="/images/memcached/memcached-distribute-3.png"></p>

<p>这样,将不同的键保存到不同的服务器上,就实现了memcached的分布式. memcached服务器增多后,键就会分散,即使一台memcached服务器发生故障
无法连接,也不会影响其他的缓存,系统依然能继续运行.</p>

<h3>4.2 余数哈希</h3>

<p>XMemcached默认使用的是Native Hash,见ArrayMemcachedSessionLocator.java:</p>

<pre><code class="java">public final long getHash(int size, String key) {
    long hash = this.hashAlgorighm.hash(key);
    return hash % size;
}
</code></pre>

<p>ArrayMemcachedSessionLocator中使用的hashAlgorighm是HashAlgorithm.NATIVE_HASH,而HashAlgorithm.NATIVE_HASH的hash()函数的实现:</p>

<pre><code class="java">public long hash(final String k) {
    long rv = 0;
    switch (this) {
    case NATIVE_HASH:
        rv = k.hashCode();
        break;
    ...
</code></pre>

<p>即XMemcached的Native Hash就是通常的余数哈希:首先求得字符串的hash值,根据该值除以服务器节点数目得到的余数决定服务器.</p>

<p>根据这种hash方式,上门这几个key分布的服务器如下(三台服务器分别为 0 1 2):</p>

<pre><code class="java">tokyo --&gt; 2
kanagawa --&gt; 1
chiba --&gt; 2
saitama --&gt; 1
gunma --&gt; 2
</code></pre>

<p><strong>优点</strong>
简单高效:计算简单,数据的分散性也相当优秀</p>

<p><strong>缺点</strong>
当添加或移除服务器时,缓存重组的代价相当巨大. 添加服务器后,余数就会产生巨变,这样就无法获取与保存时相同的服务器, 从而影响缓存的命中率.</p>

<p>下面简单验证这个:首先在3台服务器的情况下将“a”到“z”的键保存到memcached并访问的情况;接下来增加一台memcached服务器;计算缓存命中率;</p>

<p>简单测试代码(忍不住吐槽下java真是一门罗嗦的语言):</p>

<pre><code class="java">public class Hash {

    /**
     * 默认的Hash算法
     */
    private static HashAlgorithm hashAlgorithm = HashAlgorithm.NATIVE_HASH;

    /**
     * Memcached服务器数目
     */
    private static final int SERVER_SIZE = 3;

    public static void main(String[] args) {
        // 初始化
        Map&lt;Long, List&lt;String&gt;&gt; itemMap = new HashMap&lt;Long, List&lt;String&gt;&gt;();
        for (long i = 0; i &lt; SERVER_SIZE; i++) {
            itemMap.put(i, new LinkedList&lt;String&gt;());
        }

        char letters[] = new char[27];
        letters[0] = 'a';
        for (int i = 0; i &lt; 26; i++) {
            letters[i + 1] = (char) (letters[i] + 1);
            long hash = getHash(SERVER_SIZE, String.valueOf(letters[i]));
            itemMap.get(hash).add(String.valueOf(letters[i]));
        }

        for (Map.Entry&lt;Long, List&lt;String&gt;&gt; e : itemMap.entrySet()) {
            System.out.println(e.getKey() + " : " + join(e.getValue()));
        }

    }

    public static long getHash(int size, String key) {
        long hash = hashAlgorithm.hash(key);
        return hash % size;
    }

    private static String join(final List&lt;String&gt; list) {
        StringBuilder builder = new StringBuilder();
        for(String s : list) {
            builder.append(s + ' ');
        }

        return builder.toString();
    }
}
</code></pre>

<p>我们得到3台服务器(分别为0,1,2)的时候,26个字母的分布如下:</p>

<pre><code>0 : c f i l o r u x 
1 : a d g j m p s v y 
2 : b e h k n q t w z 
</code></pre>

<p>4台服务器(分别为0,1,2,3)的时候,26个字母的分布如下:</p>

<pre><code>0 : d h l p t x 
1 : a e i m q u y 
2 : b f j n r v z 
3 : c g k o s w
</code></pre>

<p>根据这两份数据,我们可以得到,加拉一台服务器之后,26个字母只有8个命中了.像这样,添加节点后键分散到的服务器会发生巨大变化.</p>

<p>同样,减少服务器(比如某一台服务器down机),也会导致大量的Miss,基本失去了缓存的作用.</p>

<p>带来的问题就是,在Web应用程序中使用memcached时, 在添加memcached服务器的瞬间缓存效率会大幅度下降,有可能会发生无法提供正常服务的情况.</p>

<h3>4.3 一致性哈希(Consistent Hashing)</h3>

<p>一致性哈希能够很大程度上(注意不是完全解决)解决余数哈希的增加服务器导致缓存失效的问题.</p>

<p><strong>4.3.1 一致性哈希原理</strong>
Consistent Hashing如下所示：首先求出memcached服务器（节点）的哈希值,
并将其配置到0～2SUP(32)的圆（continuum）上. 然后用同样的方法求出存储数据的键的哈希值,并映射到圆上. 然后从数据映射到的位置开始顺时针查找,将数据保存到找到的第一个服务器上. 如果超过2SUP(32)仍然找不到服务器,就会保存到第一台memcached服务器上.</p>

<p><img src="/images/memcached/memcached-consistent-hash1.png"></p>

<p>从上图的状态中添加一台memcached服务器.余数分布式算法由于保存键的服务器会发生巨大变化 而影响缓存的命中率,但Consistent Hashing中,只有在continuum上增加服务器的地点逆时针方向的 第一台服务器上的键会受到影响.</p>

<p><img src="/images/memcached/memcached-consistent-hash2.png"></p>

<p>因此,Consistent Hashing最大限度地抑制了键的重新分布. 而且,有的Consistent Hashing的实现方法还采用了虚拟节点的思想. 使用一般的hash函数的话,服务器的映射地点的分布非常不均匀. 因此,使用虚拟节点的思想,为每个物理节点（服务器） 在continuum上分配100～200个点.这样就能抑制分布不均匀, 最大限度地减小服务器增减时的缓存重新分布.</p>

<p><strong>4.3.2 虚拟节点</strong>
 一致性哈希算法在服务节点太少时,容易因为节点分部不均匀而造成数据倾斜问题.例如我们的系统中有两台 server,其环分布如下：</p>

<p> <img src="/images/xmemcached/virtual-node-why.png"></p>

<p> 此时必然造成大量数据集中到Server 1上,而只有极少量会定位到Server 2上.为了解决这种数据倾斜问题,一致性哈希算法引入了虚拟节点机制.</p>

<p>虚拟节点(virtual node)是实际节点(服务器)在 hash 空间的复制品,一个实际节点(服务器)对应了若干个虚拟节点,这个对应个数也称为为复制个数,虚拟节点在 hash 环中以hash值排列.</p>

<p>例如为上面的每台服务器设置三个虚拟节点:</p>

<p><img src="/images/xmemcached/virtual-node.png"></p>

<p>在实际应用中,一个物理节点对应多少的虚拟节点才能达到比较好的均衡效果,有一个效果图:</p>

<p><img src="/images/xmemcached/virtual-node-count.png"></p>

<p>纵轴为物理服务器的数目,横轴为虚拟节点的数目,可以看出,当物理服务器的数量很小时,需要更大的虚拟节点,反之则需要更少的节点,从图上可以看出,在物理服务器有10台时,差不多需要为每台服务器增加100~200个虚拟节点效果比较好.</p>

<h3>4.4 XMemcached一致性哈希的实现</h3>

<p>XMemcached Client中实现了一致性哈希,见KetamaMemcachedSessionLocator.java,使用的Hash算法是KETAMA HASH算法.</p>

<p>下面主要分析一下KetamaMemcachedSessionLocator.java代码,去掉了一些无关的代码:</p>

<p><strong>4.4.1 根据服务器列表生成Hash环</strong></p>

<pre><code class="java">    private final void buildMap(Collection&lt;Session&gt; list, HashAlgorithm alg) {
        TreeMap&lt;Long, List&lt;Session&gt;&gt; sessionMap = new TreeMap&lt;Long, List&lt;Session&gt;&gt;();

        for (Session session : list) {  //遍历每个服务器
            String sockStr = null;  //根据服务器地址得到的字符串,用于hash
            if (this.cwNginxUpstreamConsistent) {
                ... //生成sockStr
            } else {
                ... //生成sockStr
            }
            /**
             * Duplicate 160 X weight references
             */
            int numReps = NUM_REPS; //虚拟节点数目, 默认160
            //考虑权重,权重小的服务器最终生成的虚拟节点就少,数据打到这个服务器的概率就小
            if (session instanceof MemcachedTCPSession) {
                numReps *= ((MemcachedSession) session).getWeight();
            }
            if (alg == HashAlgorithm.KETAMA_HASH) {     //一致性Hash
                for (int i = 0; i &lt; numReps / 4; i++) {
                    //计算Hash值,将虚拟节点映射到当前session代表的服务器
                    byte[] digest = HashAlgorithm.computeMd5(sockStr + "-" + i);
                    for (int h = 0; h &lt; 4; h++) {
                        long k = (long) (digest[3 + h * 4] &amp; 0xFF) &lt;&lt; 24 | (long) (digest[2 + h * 4] &amp; 0xFF) &lt;&lt; 16
                                | (long) (digest[1 + h * 4] &amp; 0xFF) &lt;&lt; 8 | digest[h * 4] &amp; 0xFF;
                        this.getSessionList(sessionMap, k).add(session);
                    }

                }
            } else {
                ...
            }
        }
        this.ketamaSessions = sessionMap;
        this.maxTries = list.size();
    }

    private List&lt;Session&gt; getSessionList(TreeMap&lt;Long, List&lt;Session&gt;&gt; sessionMap, long k) {
        List&lt;Session&gt; sessionList = sessionMap.get(k);
        if (sessionList == null) {
            sessionList = new ArrayList&lt;Session&gt;();
            sessionMap.put(k, sessionList);
        }
        return sessionList;
    }
</code></pre>

<p>buildMap的结果就是在0-2<sup>32</sup>这个圈上生成一些列的虚拟节点,每个虚拟节点都指向一个真实服务器.</p>

<p>例如本机只开一个服务器情况下,没有设置权重,会生成160个虚拟节点,每个虚拟节点都指向中一台服务器:</p>

<p><img src="/images/xmemcached/xmemcached-ketama-buildmap.png">
<strong>4.4.2 对Key做Hash</strong></p>

<p>做get或者set的时候,首先都会通过key找到服务器:</p>

<pre><code class="java">public Session getSessionByKey(final String key);
</code></pre>

<p>其实现代码如下:</p>

<pre><code class="java">//通过key找真实到服务器
public final Session getSessionByKey(final String key) {
    if (this.ketamaSessions == null || this.ketamaSessions.size() == 0) {
        return null;
    }
    long hash = this.hashAlg.hash(key);         //计算key的Hash值
    Session rv = this.getSessionByHash(hash);   //通过Hash值找服务器
    int tries = 0;
    while (!this.failureMode &amp;&amp; (rv == null || rv.isClosed())
            &amp;&amp; tries++ &lt; this.maxTries) {
        hash = this.nextHash(hash, key, tries);
        rv = this.getSessionByHash(hash);
    }
    return rv;
}

public final Session getSessionByHash(final long hash) {
    TreeMap&lt;Long, List&lt;Session&gt;&gt; sessionMap = this.ketamaSessions;
    if (sessionMap.size() == 0) {
        return null;
    }
    Long resultHash = hash;
    if (!sessionMap.containsKey(hash)) {
        //下面的逻辑是找到大于hash值的第一个虚拟节点(即ceiling)
        //Java 1.6使用ceilingKey可以实现,为兼容jdk5,使用tailMap,tailMap为所有大于hash值的虚拟节点
        SortedMap&lt;Long, List&lt;Session&gt;&gt; tailMap = sessionMap.tailMap(hash);
        if (tailMap.isEmpty()) {//没有比hash值大的节点,则取第一个虚拟节点
            resultHash = sessionMap.firstKey(); 
        } else {
            resultHash = tailMap.firstKey();//取tailMap第一个,即最大于hash值最小节点
        }
    }

    List&lt;Session&gt; sessionList = sessionMap.get(resultHash);
    if (sessionList == null || sessionList.size() == 0) {
        return null;
    }
    int size = sessionList.size();
    return sessionList.get(this.random.nextInt(size));
}
</code></pre>

<p>这里调试时候key是'a',其Hash值是3111502092:</p>

<p><img src="/images/xmemcached/xmemcached-ketama-key.png">
getSessionByHash的逻辑其实就是在Hash环上找第一个大于key对应hash值的虚拟节点,通过虚拟节点找到真实服务器.</p>

<p>这里的tailMap即为大于3111502092的虚拟节点,如下:</p>

<p><img src="/images/xmemcached/xmemcached-ketama-tailMap.png"></p>

<p>因此我们要找的虚拟节点就是3164521287对应的虚拟节点,其服务器指向的是127.0.0.1:11211.因此当前key(这里为'a')的请求被打到这台服务器上.</p>

<h3>4.5 使用XMemcached的一致性哈希</h3>

<p>默认情况下XMemcached使用的是余数哈希,如下使用一致性哈希:</p>

<pre><code>    MemcachedClientBuilder builder = new XMemcachedClientBuilder(AddrUtil.getAddresses(servers));
    // 设置使用一致性hash
    builder.setSessionLocator(new KetamaMemcachedSessionLocator());
    return builder.build();
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Memcached内存存储]]></title>
    <link href="http://xiaobaoqiu.github.io/blog/2014/10/15/memcachednei-cun-cun-chu/"/>
    <updated>2014-10-15T18:13:49+08:00</updated>
    <id>http://xiaobaoqiu.github.io/blog/2014/10/15/memcachednei-cun-cun-chu</id>
    <content type="html"><![CDATA[<p>早就听说过Memcached独特的内存管理方式,写着篇文章的目的就是了解Memcached的内存管理,学习其源代码.</p>

<h1>1.什么是Slab Allocator</h1>

<p>memcached默认情况下采用了名为Slab Allocator的机制分配、管理内存，Slab Allocator的基本原理是按照预先规定的大小，将分配的内存分割成特定长度的块，以期望完全解决内存碎片问题。而且，slab allocator还有重复使用已分配的内存的目的。 也就是说，分配到的内存不会释放，而是重复利用。</p>

<p><img src="/images/memcached/memcached-slab.png"></p>

<h1>2.Slab Allocation的主要术语</h1>

<pre><code>Page        分配给Slab的内存空间,默认是1MB,分配给Slab之后根据slab的大小切分成chunk
Chunk       用于缓存记录的内存空间
Slab Class  特定大小的chunk的组
</code></pre>

<p><img src="/images/memcached/memcached-slab-page.png"></p>

<h1>3.Slab初始化</h1>

<p>在Memcached启动时候会调用slab的初始化代码(详见memcached.c中main函数调用slabs_init函数).</p>

<p>slabs_init函数声明:</p>

<pre><code class="c">/** Init the subsystem. 1st argument is the limit on no. of bytes to allocate,
    0 if no limit. 2nd argument is the growth factor; each slab will use a chunk
    size equal to the previous slab's chunk size times this factor.
    3rd argument specifies if the slab allocator should allocate all memory
    up front (if true), or allocate memory in chunks as it is needed (if false)
*/
void slabs_init(const size_t limit, const double factor, const bool prealloc);
</code></pre>

<p>其中limit表示memcached最大使用内存;factor表示slab中chunk size的增长因子,slab中chunk size的大小等于前一个slab的chunk size乘以factor;</p>

<p>memcached.c中main函数调用slabs_init函数:</p>

<pre><code class="c">slabs_init(settings.maxbytes, settings.factor, preallocate);
</code></pre>

<p>其中settings.maxbytes默认值为64M,启动memcached使用选项-m设置;settings.factor默认为1.25,启动memcached时候使用-f设置;preallocate指的是启动memcached的时候默认为每种类型slab预先分配一个page的内存,默认是false;</p>

<pre><code>settings.maxbytes = 64 * 1024 * 1024; /* default is 64MB */
...
settings.factor = 1.25;
...
preallocate = false
</code></pre>

<p>slabs_init函数实现:</p>

<pre><code class="c">/**
 * Determines the chunk sizes and initializes the slab class descriptors
 * accordingly.
 */
void slabs_init(const size_t limit, const double factor, const bool prealloc) {
    int i = POWER_SMALLEST - 1;
    //真实占用大小=对象大小+48
    unsigned int size = sizeof(item) + settings.chunk_size;

    mem_limit = limit;

    //开启预分配,则首先将limit大小(默认64M)的内存全部申请
    if (prealloc) {
        /* Allocate everything in a big chunk with malloc */
        mem_base = malloc(mem_limit);
        if (mem_base != NULL) {
            mem_current = mem_base;
            mem_avail = mem_limit;
        } else {
            fprintf(stderr, "Warning: Failed to allocate requested memory in"
                    " one large chunk.\nWill allocate in smaller chunks\n");
        }
    }

    //清空所有的slab
    memset(slabclass, 0, sizeof(slabclass));

    while (++i &lt; POWER_LARGEST &amp;&amp; size &lt;= settings.item_size_max / factor) {
        /* Make sure items are always n-byte aligned */
        if (size % CHUNK_ALIGN_BYTES)
            size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES);

        slabclass[i].size = size;
        slabclass[i].perslab = settings.item_size_max / slabclass[i].size;
        size *= factor;
        if (settings.verbose &gt; 1) {
            fprintf(stderr, "slab class %3d: chunk size %9u perslab %7u\n",
                    i, slabclass[i].size, slabclass[i].perslab);
        }
    }

    //最大chunksize的一个slab,chunksize为settings.item_size_max(默认1M)
    power_largest = i;
    slabclass[power_largest].size = settings.item_size_max;
    slabclass[power_largest].perslab = 1;
    if (settings.verbose &gt; 1) {
        fprintf(stderr, "slab class %3d: chunk size %9u perslab %7u\n",
                i, slabclass[i].size, slabclass[i].perslab);
    }

    //记录已分配的空间大小
    /* for the test suite:  faking of how much we've already malloc'd */
    {
        char *t_initial_malloc = getenv("T_MEMD_INITIAL_MALLOC");
        if (t_initial_malloc) {
            mem_malloced = (size_t)atol(t_initial_malloc);
        }
    }

    //开启了预分配,则为每种slab都分配一个page的空间
    if (prealloc) {
        slabs_preallocate(power_largest);
    }
}
</code></pre>

<p>其中settings.chunk_size默认为48:</p>

<pre><code>settings.chunk_size = 48;         /* space for a modest key and value */
</code></pre>

<p>POWER_LARGEST指slab种类的最大值,默认只为200,在memcached.c中设置</p>

<pre><code>#define POWER_LARGEST  200
</code></pre>

<p>settings.item_size_max就是每个page的大小,默认1M,在memcached.c中初始化:</p>

<pre><code>settings.item_size_max = 1024 * 1024; /* The famous 1MB upper limit. */
</code></pre>

<p>默认不开启预分配,因为很多时候Memcached只存储一种类型的数据(即其大小相对比较固定),这时候其他类型的预分配的slab空间就会浪费.</p>

<p>预分配的逻辑就是从最小的slab开始,为每类slab分配一个Page大小的空间(空间不足时停止分配):</p>

<pre><code class="c">static void slabs_preallocate (const unsigned int maxslabs) {
    int i;
    unsigned int prealloc = 0;

    /* pre-allocate a 1MB slab in every size class so people don't get
       confused by non-intuitive "SERVER_ERROR out of memory"
       messages.  this is the most common question on the mailing
       list.  if you really don't want this, you can rebuild without
       these three lines.  */

    for (i = POWER_SMALLEST; i &lt;= POWER_LARGEST; i++) {
        if (++prealloc &gt; maxslabs)
            return;
        if (do_slabs_newslab(i) == 0) {
            fprintf(stderr, "Error while preallocating slab memory!\n"
                "If using -L or other prealloc options, max memory must be "
                "at least %d megabytes.\n", power_largest);
            exit(1);
        }
    }

}
</code></pre>

<p>do_slabs_newslab的工作就是为某一个slab分配空间,并将空间划分乘固定大小的chunk:</p>

<pre><code class="c">static int do_slabs_newslab(const unsigned int id) {
    slabclass_t *p = &amp;slabclass[id];
    int len = settings.slab_reassign ? settings.item_size_max
        : p-&gt;size * p-&gt;perslab;
    char *ptr;

    if ((mem_limit &amp;&amp; mem_malloced + len &gt; mem_limit &amp;&amp; p-&gt;slabs &gt; 0) ||
        (grow_slab_list(id) == 0) ||
        ((ptr = memory_allocate((size_t)len)) == 0)) {  //申请内存

        MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id);
        return 0;
    }

    memset(ptr, 0, (size_t)len);
    //将内存划分乘chunk
    split_slab_page_into_freelist(ptr, id);

    //维护slab链表
    p-&gt;slab_list[p-&gt;slabs++] = ptr;
    mem_malloced += len;
    MEMCACHED_SLABS_SLABCLASS_ALLOCATE(id);

    return 1;
}
</code></pre>

<p>split_slab_page_into_freelist的主要控制就是Page划分乘chunk并清空:</p>

<pre><code class="c">static void split_slab_page_into_freelist(char *ptr, const unsigned int id) {
    slabclass_t *p = &amp;slabclass[id];
    int x;
    for (x = 0; x &lt; p-&gt;perslab; x++) {
        do_slabs_free(ptr, 0, id);
        ptr += p-&gt;size;
    }
}
</code></pre>

<p>memcached的内存分配策略就是：按slab需求分配page,各slab按需使用chunk存储.</p>

<p>按需分配的意思就是某一类slab没有对象可存,就不会分配(非preallocate模式),某类slab存储对象很多,就会分配多个slab形成链表.</p>

<p>这里有几个特点要注意:</p>

<pre><code>1.Memcached分配出去的page不会被回收或者重新分配;
2.Memcached申请的内存不会被释放;
3.slab空闲的chunk不会借给任何其他slab使用(新版本memcached有slab_reassign,slab_automove的功能);
</code></pre>

<p>slab内存结构图,二维数组链表:</p>

<p><img src="/images/memcached/memcached_slab_alloct.jpg"></p>

<h1>4.往Slab中缓存记录</h1>

<p>memcached根据收到的数据的大小,选择最适合数据大小的slab.
memcached中保存着slab内空闲chunk的列表,根据该列表选择chunk,
然后将数据缓存于其中.</p>

<p><img src="/images/memcached/memcached-slab-save.png"></p>

<p>代码如下:</p>

<pre><code class="c">/*
 * Figures out which slab class (chunk size) is required to store an item of
 * a given size.
 *
 * Given object size, return id to use when allocating/freeing memory for object
 * 0 means error: can't store such a large object
 */
unsigned int slabs_clsid(const size_t size) {
    int res = POWER_SMALLEST;   //最小slab编号

    if (size == 0)
        return 0;
    while (size &gt; slabclass[res].size)
        if (res++ == power_largest)     /* won't fit in the biggest slab */
            return 0;
    return res;
}
</code></pre>

<p>参数是待存储对象的大小,根据这个大小,从最小的Chunk Size开始查找,找到第一个(即最小的)能放下size大小的对象的Chunk.找不到(size大于最大的Chunk Size)返回0(这就是为什么slab class从1开始而不是从0开始).</p>

<p>如果某个Slab没有剩余的Chunk了，系统便会给这个Slab分配一个新的Page以供使用，如果没有Page可用，系统就会触发LRU机制，通过删除冷数据来为新数据腾出空间，这里有一点需要注意的是：LRU不是全局的，而是针对Slab而言的.</p>

<p>slab内存分配示例:</p>

<p><img src="/images/memcached/memcached_slab_ins.jpg"></p>

<h1>5.Slab Allocator的缺点</h1>

<p>由于Slab Allocator分配的是特定长度的内存，因此无法有效利用分配的内存。 例如，将100字节的数据缓存到128字节的chunk中，剩余的28字节就浪费了。</p>

<p><img src="/images/memcached/memcached-slab-disadvantage.png"></p>

<h1>6.Memcached减少内存浪费</h1>

<h3>4.1:调整growth factor</h3>

<pre><code>(1).估算我们item的大小
key键长＋suffix+value值长＋结构大小(48字节)
(2).逐步调整growth factor,使得某个slab的大小和我们的item大小接近(必须大于我们item的大小)
</code></pre>

<h1>7.过期数据</h1>

<pre><code>(1).LRU过期策略;
(2).在slab级别上执行LRU策略;
(3).查看是否过去是在get的时候,即懒惰(lazy)检查;
</code></pre>

<h1>8.memcached-tool脚本</h1>

<p>memcached-tool脚本可以方便地获得slab的使用情况 （它将memcached的返回值整理成容易阅读的格式）,可以从下面的地址获得脚本:
<a href="http://www.netingcn.com/demo/memcached-tool.zip">http://www.netingcn.com/demo/memcached-tool.zip</a></p>

<p>使用方法也极其简单：</p>

<pre><code>perl memcached-tool server_ip:prot option
</code></pre>

<p>比如:</p>

<pre><code>perl memcached-tool 10.0.0.5:11211 display    # shows slabs
perl memcached-tool 10.0.0.5:11211            # same.  (default is display)
perl memcached-tool 10.0.0.5:11211 stats      # shows general stats
perl memcached-tool 10.0.0.5:11211 move 7 9   # takes 1MB slab from class #7
                                              # to class #9.
</code></pre>

<p>输出示例:</p>

<pre><code>#  Item_Size   Max_age  1MB_pages Count   Full?
 1     104 B  1394292 s    1215 12249628    yes
 2     136 B  1456795 s      52  400919     yes
 ...
</code></pre>

<p>各列的含义为：</p>

<pre><code>#           slab class编号
Item_Size   Chunk大小
Max_age     LRU内最旧的记录的生存时间
1MB_pages   分配给Slab的页数
Count       Slab内的记录数
Full?       Slab内是否含有空闲chunk
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Memcached]]></title>
    <link href="http://xiaobaoqiu.github.io/blog/2014/09/23/memcached/"/>
    <updated>2014-09-23T00:08:56+08:00</updated>
    <id>http://xiaobaoqiu.github.io/blog/2014/09/23/memcached</id>
    <content type="html"><![CDATA[<p>最近项目中的图片上传那块带来很大麻烦,原因是因为业务逻辑的问题,会导致统一张图片上传很多次(平均5次上传有4次是重复的),导致图片服务器压力很大.</p>

<p>直观想法是使用KV缓存,第一个想到的是Memcached.</p>

<p>其实很长时间之前就学习过一段时间的Memcached,但是一致没有整理,趁这次机会,整理一下Memcached相关的知识.</p>

<h1>1.Memcached是什么</h1>

<p>Memcached官方:<a href="http://memcached.org/">http://memcached.org/</a></p>

<p>Memcached wiki:<a href="http://code.google.com/p/memcached/wiki/NewStart">http://code.google.com/p/memcached/wiki/NewStart</a></p>

<p>Memcached是一个免费的开源的,高性能的,分布式的内存KV(key-value)缓存系统,一般通过缓解数据库压力来达到加速动态Web应用程序的目的.可以存储任意类型的小数据块,比如数据库查询结果,接口调用结果等.</p>

<p>Memcached可以称之为简单高效.它简单的设计保证了快速发布,使用者开发简单,面临大量数据缓存的时候能够解决很多问题.同时提供了绝大多数流行语言的API接口.</p>

<p>许多Web应用都将数据保存到RDBMS中，应用服务器从中读取数据并在浏览器中显示。 但随着数据量的增大、访问的集中，就会出现RDBMS的负担加重、数据库响应恶化、 网站显示延迟等重大影响。</p>

<p>这时就该memcached大显身手了。一般的使用目的是，通过缓存数据库查询结果，减少数据库访问次数，以提高动态Web应用的速度、提高可扩展性。</p>

<p><img src="/images/memcached/usage.png"></p>

<h1>2.Memcached特性</h1>

<p>memcached作为高速运行的分布式缓存服务器，具有以下的特点:
1. 协议简单
2. 基于libevent的事件处理
3. 内置内存存储方式
4. memcached不互相通信的分布式
下面分别介绍.</p>

<h3>2.1协议简单</h3>

<p>Memcached使用简单的基于文本行的协议。因此，通过telnet 也能在memcached上保存数据、取得数据。如下:</p>

<pre><code>xiaobaoqiu@xiaobaoqiu:~$ telnet 127.0.0.1 11211
Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is '^]'.
add myid 1 0 4
1234
STORED
get myid
VALUE myid 1 4
1234
END
</code></pre>

<p>协议文档位于memcached的源代码内,可以参考:
<a href="https://github.com/memcached/memcached/blob/master/doc/protocol.txt">https://github.com/memcached/memcached/blob/master/doc/protocol.txt</a></p>

<h3>2.2基于libevent的事件处理</h3>

<p>libevent是个程序库，它将Linux的epoll、BSD类操作系统的kqueue等事件处理功能
封装成统一的接口。即使对服务器的连接数增加，也能发挥O(1)的性能。 memcached使用这个libevent库，因此能在Linux、BSD、Solaris等操作系统上发挥其高性能。</p>

<p>关于libevent参考:
<a href="http://libevent.org/">http://libevent.org/</a>
<a href="http://www.kegel.com/c10k.html">http://www.kegel.com/c10k.html</a></p>

<h3>2.3内置内存存储方式</h3>

<p>为了提高性能，memcached中保存的数据都存储在memcached内置的内存存储空间中。由于数据仅存在于内存中，因此重启memcached、重启操作系统会导致全部数据消失。另外，内容容量达到指定值之后，就基于LRU(Least Recently Used)算法自动删除不使用的缓存。</p>

<p>memcached本身是为缓存而设计的服务器，因此并没有过多考虑数据的永久性问题。</p>

<h3>2.4memcached不互相通信的分布式</h3>

<p>memcached虽然号称“分布式”缓存服务器，但服务器端并没有分布式功能。
各个memcached不会互相通信以共享信息。那么，怎样进行分布式呢？ 这完全取决于客户端的实现。</p>

<p><img src="/images/memcached/client.png"></p>

<h1>3.Memcached安装</h1>

<p>所有可下载版本见:
<a href="http://code.google.com/p/memcached/wiki/ReleaseNotes">http://code.google.com/p/memcached/wiki/ReleaseNotes</a></p>

<p>直接按照官网安装就可以,下面直接安装最新的版本,需要已经安装了libevent:</p>

<pre><code>wget http://memcached.org/latest
tar -zxvf memcached-1.x.x.tar.gz
cd memcached-1.x.x
./configure &amp;&amp; make &amp;&amp; make test &amp;&amp; sudo make install
</code></pre>

<p>memcached的命令参数见-h,从中也可以看出memcached的版本:</p>

<pre><code>xiaobaoqiu@xiaobaoqiu:~$ memcached -h
memcached 1.4.14
-p &lt;num&gt;      TCP监听端口TCP(默认: 11211)
-U &lt;num&gt;      UDP监听端口TCP(默认: 11211, 0表示关闭)
-s &lt;file&gt;     用于监听的UNIX套接字路径（禁用网络支持）
-a &lt;mask&gt;     UNIX套接字访问掩码，八进制数字(default: 0700)
-l &lt;addr&gt;     监听的IP地址(默认：INADDR_ANY，所有地址), &lt;addr&gt; 形如host:port.
-d            以守护进程(daemon)形式运行
-r            最大核心文件(core file)限制
-u &lt;username&gt; 设定进程所属用户（只有root用户可以使用这个参数）
-m &lt;num&gt;      最大使用内存,单位MB(默认: 64 MB)
-M            当内存耗尽时候返回错误 (而不是移除过期项)
-c &lt;num&gt;      并发连接的最大数目(默认: 1024)
-k            锁定所有内存页。注意你可以锁定的内存是有上限的.
              试图分配更多内存会失败的，所以留意启动守护进程时所用的用户可分配的内存上限(不是前面的 -u &lt;username&gt; 参数；在sh下，使用命令"ulimit -S -l NUM_KB"来设置).
-v            提示信息（在事件循环中打印错误/警告信息）
-vv           详细信息（还打印客户端命令/响应）
-vvv          超详细信息（还打印内部状态的变化）
-h            帮助信息
-i            打印memcached版本和libevent版本
-P &lt;file&gt;     将进程id保存在文件中,只在-d选项下有效
-f &lt;factor&gt;   块大小增长因子(默认: 1.25)
-n &lt;bytes&gt;    chunk(key+value+flags)的最小空间 (默认: 48)
              (chunk数据结构本身需要消耗48个字节，所以一个chunk实际消耗的内存是n+48)
-L            尝试使用大内存页(如果可以的话). 增加内存页大小能够减少页表缓冲(TLB)
              丢失次数,提高效率.为了从操作系统获取大内存页,memcached会把全部数据项分配到一个大区块
-D &lt;char&gt;     使用 &lt;char&gt; 作为前缀和ID的分隔符.
              这个用于按前缀获得状态报告。默认是":"（冒号）.
              如果指定了这个参数，则状态收集会自动开启；如果没指定，则需要用命令"stats detail on"来开启。
-t &lt;num&gt;      使用的线程数目 (默认: 4)
-R            每个事件的最大请求数目,即每个连接处理的最大请求数目(默认: 20)
-C            禁用CAS
-b            设置积压队列(backlog queue)的最大限制(默认: 1024)
-B            绑定协议,可选值：ascii,binary,auto（默认）
-I            重写每个数据页尺寸。调整数据项最大尺寸(默认: 1mb, min: 1k, max: 128m)
-S            Turn on Sasl authentication
-o            Comma separated list of extended or experimental options
              - (EXPERIMENTAL) maxconns_fast: immediately close new
                connections if over maxconns limit
              - hashpower: An integer multiplier for how large the hash
                table should be. Can be grown at runtime if not big enough.
                Set this based on "STAT hash_power_level" before a 
                restart.
</code></pre>

<p>启动memcached时候可以带上这些选项,启动memcached:</p>

<pre><code>xiaobaoqiu@xiaobaoqiu:~$ memcached -d
</code></pre>

<p>需要注意的是,还有一个memcached.conf文件存在(通过whereis memcached可以看得到),他是memcached默认的配置参数,其内容就是我们上面介绍的memcached启动参数,我的文件如下:</p>

<pre><code># memcached default config file
# 2003 - Jay Bonci &lt;jaybonci@debian.org&gt;
# This configuration file is read by the start-memcached script provided as
# part of the Debian GNU/Linux distribution. 

# Run memcached as a daemon. This command is implied, and is not needed for the
# daemon to run. See the README.Debian that comes with this package for more
# information.
# 以守护进程的形似运行
-d

# Log memcached's output to /var/log/memcached
# 日志文件存放位置
logfile /var/log/memcached.log

# Be verbose
# -v

# Be even more verbose (print client commands as well)
# -vv

# Start with a cap of 64 megs of memory. It's reasonable, and the daemon default
# Note that the daemon will grow to this size, but does not start out holding this much
# memory
# 分配给mencached的内存数目，单位是MB
-m 256

# Default connection port is 11211
# Memcached的监听端口
-p 11211

# Run the daemon as root. The start-memcached will default to running as root if no
# -u command is present in this config file
# 允许memcached的用户
-u memcache

# Specify which IP address to listen on. The default is to listen on all IP addresses
# This parameter is one of the only security measures that memcached has, so make sure
# it's listening on a firewalled interface.
# 监听的服务器IP地址
-l 127.0.0.1

# Limit the number of simultaneous incoming connections. The daemon default is 1024
# -c 1024

# Lock down all paged memory. Consult with the README and homepage before you do this
# -k

# Return error when memory is exhausted (rather than removing items)
# -M

# Maximize core file limit
# -r
</code></pre>

<p>我们启动memecached默认会带上这个文件的参数,可以通过ps命令看我们memcached启动的参数:</p>

<pre><code>xiaobaoqiu@xiaobaoqiu:~$ ps -ef | grep memcached
memcache  1241     1  0 10:38 ?        00:00:00 /usr/bin/memcached -m 256 -p 11211 -u memcache -l 127.0.0.1
</code></pre>

<p>可以使用telnet连接上memcached,之后就可以输入memcached的命令了:</p>

<pre><code>xiaobaoqiu@xiaobaoqiu:~$ telnet 127.0.0.1 11211
Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is '^]'.
</code></pre>

<h1>4.Memcached命令</h1>

<p>介绍Memcached的常用命令,不断补充中,官方文档参考:
<a href="https://code.google.com/p/memcached/wiki/NewCommands">https://code.google.com/p/memcached/wiki/NewCommands</a>
按照命令左右,简单分为存储,删除,自增自减,读取和状态四类命令</p>

<h3>4.1存储命令</h3>

<p>命令:add set replace
格式:</p>

<pre><code>&lt;command name&gt; &lt;key&gt; &lt;flags&gt; &lt;exptime&gt; &lt;bytes&gt;

&lt;data block&gt;
</code></pre>

<p>其中参数command name指操作命令,即set/add/replace;key指缓存的键值;flags指客户机使用它存储关于键值对的额外信息,exptime缓存过期时间,单位为秒,0表示永远存储;bytes缓存值的字节数;data block指数据块.</p>

<h5>(1).set:无论如何都添加或更新的命令(key不存在则添加,存在则更新)</h5>

<p>下面的示例包含了key存在和不存在两种情况:</p>

<pre><code>xiaobaoqiu@xiaobaoqiu:~$ telnet 127.0.0.1 11211
Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is '^]'.
set name 0 0 11
baoqiu.xiao
STORED
get name
VALUE name 0 11
baoqiu.xiao
END
set name 0 0 9 
memcached
STORED
get name
VALUE name 0 9
memcached
END
</code></pre>

<h5>(2).add:只有数据不存在时添加值的add命令</h5>

<p>已经存在的key是不能再add
<code>
xiaobaoqiu@xiaobaoqiu:~$ telnet 127.0.0.1 11211
Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is '^]'.
add sex 0 0 1
f
STORED
add sex 0 0 1
m
NOT_STORED
</code></p>

<h5>(3).replace:只有数据存在时替换的replace命令</h5>

<p>只有存在才能replace:</p>

<pre><code>xiaobaoqiu@xiaobaoqiu:~$ telnet 127.0.0.1 11211
Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is '^]'.
replace host 0 0 9
localhost
NOT_STORED
add host 0 0 9
127.0.0.1
STORED
get host
VALUE host 0 9
127.0.0.1
END
replace host 0 0 9
localhost
STORED
get host 
VALUE host 0 9
localhost
END
</code></pre>

<h3>4.2删除命令</h3>

<h5>(1).delete:删除已经存在的数据</h5>

<pre><code>xiaobaoqiu@xiaobaoqiu:~$ telnet 127.0.0.1 11211
Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is '^]'.
delete unknow
NOT_FOUND
add unknow 0 0 6
unknow
STORED
delete unknow
DELETED
</code></pre>

<h3>4.3自增自减命令</h3>

<p>incr和decr命令,注意只能在数字类型上进行自增自减操作
<code>
xiaobaoqiu@xiaobaoqiu:~$ telnet 127.0.0.1 11211
Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is '^]'.
set age 2 0 2
25
STORED
incr age 1
26
incr age 3
29
decr age 2
27
set name 1 0 11
baoqiu.xiao
STORED
incr name 1
CLIENT_ERROR cannot increment or decrement non-numeric value
</code></p>

<h3>4.4读取命令</h3>

<h5>(1).get:获取一条或者多条数据</h5>

<pre><code>xiaobaoqiu@xiaobaoqiu:~$ telnet 127.0.0.1 11211
Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is '^]'.
add name 1 0 11
baoqiu.xiao
STORED
add age 2 0 2
25
STORED
get name age
VALUE name 1 11
baoqiu.xiao
VALUE age 2 2
25
END
</code></pre>

<h5>(2).gets:获取一条或者多条数据</h5>

<p>gets命令比get返回的值多一个数字(类似于版本号)用来判断数据是否发生过改变.
<code>
gets name age
VALUE name 1 11 12
baoqiu.xiao
VALUE age 2 2 13
25
END
</code></p>

<h5>(3).cas:意思是check and set</h5>

<p>只有当最后一个参数和gets获取的那个用来判断数据发生改变的那个值相同时才会存储成功，否则返回 exists.</p>

<pre><code>xiaobaoqiu@xiaobaoqiu:~$ telnet 127.0.0.1 11211
Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is '^]'.
gets name
VALUE name 1 11 19
baoqiu.xiao
END
cas name 1 0 9 20
memcached
EXISTS
cas name 1 0 9 19
memcached
STORED
</code></pre>

<p>gets返回的版本参数为19,cas的最后一个参数必须为19才能设置成功.</p>

<h3>4.5状态命令</h3>

<h5>(1).stat:显示memcachd状态</h5>

<p>包括使用的内存大小等关键信息,下面包含各个项的意义:</p>

<pre><code>xiaobaoqiu@xiaobaoqiu:~$ telnet 127.0.0.1 11211
Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is '^]'.
stats
STAT pid 1241                   //memcached服务进程的进程ID
STAT uptime 24026               //memcached服务从启动到当前所经过的时间，单位是秒
STAT time 1411579124            //memcached服务器所在主机当前系统的时间，单位是秒
STAT version 1.4.14             //memcached组件的版本
STAT libevent 2.0.21-stable     //libevent版本
STAT pointer_size 64            //服务器所在主机操作系统的指针大小，一般为32或64
STAT rusage_user 0.404722       //进程累计使用用户时间
STAT rusage_system 0.313086     //进程累计使用系统时间
STAT curr_connections 5         //当前系统打开的连接数
STAT total_connections 21       //从memcached服务启动开始，系统打开过的连接总数
STAT connection_structures 6    //从memcached服务启动到现在，被服务器分配的连接结构数量
STAT reserved_fds 20            //
STAT cmd_get 15                 //累积get命令次数
STAT cmd_set 22                 //累积set命令次数
STAT cmd_flush 0                //累积flush命令次数
STAT cmd_touch 0                //
STAT get_hits 9                 //命中次数,即get成功的次数
STAT get_misses 6               //miss次数,即get失败的次数
STAT delete_misses 2            //delete miss次数
STAT delete_hits 5              //delete 命中次数
STAT incr_misses 0              //incr miss次数
STAT incr_hits 2                //incr 命中次数
STAT decr_misses 0              //decr miss次数
STAT decr_hits 1                //decr命中次数
STAT cas_misses 0               //cas命令miss的次数
STAT cas_hits 1                 //cas命令命中的次数
STAT cas_badval 1               //使用cas擦拭次数
STAT touch_hits 0
STAT touch_misses 0
STAT auth_cmds 0
STAT auth_errors 0
STAT bytes_read 968             //memcached服务器从网络读取的总的字节数
STAT bytes_written 706          //memcached服务器发送到网络的总的字节数
STAT limit_maxbytes 268435456   //memcached允许使用的最大字节,256M,见memcached.conf
STAT accepting_conns 1          //目前使用的连接数
STAT listen_disabled_num 0
STAT threads 4                  //工作线程的总数量
STAT conn_yields 0
STAT hash_power_level 16
STAT hash_bytes 524288
STAT hash_is_expanding 0
STAT expired_unfetched 0
STAT evicted_unfetched 0
STAT bytes 373                  //系统存储缓存对象所使用的存储空间，单位为字节
STAT curr_items 5               //当前缓存中存放的所有缓存对象的数量,不包括已删除的
STAT total_items 22             //自启动起所有缓存对象的数量,包括已删除的
STAT evictions 0                //从缓存移除的缓存对象数目,包括过期和空间不足时LRU移除
STAT reclaimed 0
END
</code></pre>

<h1>5.Memcached客户端</h1>

<p>许多语言都实现了连接memcached的客户端,见官网:
<a href="https://code.google.com/p/memcached/wiki/Clients">https://code.google.com/p/memcached/wiki/Clients</a></p>

<p>主流的语言基本都支持:</p>

<pre><code>C/C++, PHP, Java, Python, Ruby, Perl, .Net, Erlang, Lua等
</code></pre>

<p>这里以Java为例,典型的三种Client,其中spymemcached和Xmemcached:
  (1).Memcached-Java-Client
  <a href="https://github.com/gwhalin/Memcached-Java-Client">https://github.com/gwhalin/Memcached-Java-Client</a>
  (2).spymemcached
  <a href="http://code.google.com/p/spymemcached/">http://code.google.com/p/spymemcached/</a>
  (3).Xmemcached
  <a href="https://code.google.com/p/xmemcached/">https://code.google.com/p/xmemcached/</a></p>

<h3>5.1性能比较</h3>

<p>java memcached client官方发布的性能对比：<a href="https://github.com/gwhalin/Memcached-Java-Client/wiki/PERFORMANCE">https://github.com/gwhalin/Memcached-Java-Client/wiki/PERFORMANCE</a></p>

<p>XMemcached官方发布的性能对比：<a href="http://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html">http://xmemcached.googlecode.com/svn/trunk/benchmark/benchmark.html</a></p>

<p>根据网络资料,Xmemcached是一个不错的选择,无论在低并发还是高并发访问的情况下，都可以保持一个比较优秀的性能表现.</p>

<h3>5.2Xmemcache</h3>

<p>maven支持:<a href="http://mvnrepository.com/artifact/com.googlecode.xmemcached/xmemcached">http://mvnrepository.com/artifact/com.googlecode.xmemcached/xmemcached</a>
<code>
&lt;dependency&gt;
  &lt;groupId&gt;com.googlecode.xmemcached&lt;/groupId&gt;
  &lt;artifactId&gt;xmemcached&lt;/artifactId&gt;
  &lt;version&gt;1.4.1&lt;/version&gt;
&lt;/dependency&gt;
</code></p>

<p>简单使用:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>//memcached服务的IP及端口号
</span><span class='line'>String addr=&ldquo;192.168.100.112:11211 192.168.100.113:11211&rdquo;
</span><span class='line'>MemcachedClientBuilder builder = new XMemcachedClientBuilder(AddrUtil.getAddresses(addr));
</span><span class='line'>MemcachedClient client = builder.build();&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;String cacheKey = buildCacheKey();        //key
</span><span class='line'>Object cacheObject = buildCacheObject();  //value
</span><span class='line'>cacheClient.set(cacheKey, exp, cacheObject);  //memcached set</span></code></pre></td></tr></table></div></figure></p>
]]></content>
  </entry>
  
</feed>
